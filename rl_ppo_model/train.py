#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
RL PPOè®­ç»ƒè„šæœ¬ - ä½¿ç”¨B2Dæ•°æ®é›†è¿›è¡Œè¡Œä¸ºå…‹éš†é¢„è®­ç»ƒ
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import json
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/ajifang/b2drive')
sys.path.append('/home/ajifang/b2drive/rl_ppo_model')

# å¯¼å…¥æ•°æ®åŠ è½½å™¨
from rl_ppo_model.dataset import create_dataloader


class BehaviorCloningTrainer:
    """
    è¡Œä¸ºå…‹éš†è®­ç»ƒå™¨ - ç”¨äºPPOé¢„è®­ç»ƒ
    """

    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = Path(config['save_dir'])
        self.save_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºç½‘ç»œ
        self._build_model()

        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.policy_network.parameters(),
            lr=config['learning_rate']
        )

        # æŸå¤±å‡½æ•°
        self.criterion = nn.MSELoss()

        # è®­ç»ƒç»Ÿè®¡
        self.train_losses = []
        self.epoch_metrics = []

    def _build_model(self):
        """æ„å»ºç­–ç•¥ç½‘ç»œ"""
        print("ğŸ—ï¸  æ„å»ºç­–ç•¥ç½‘ç»œ...")

        # ä½¿ç”¨ä½ çš„PPOç½‘ç»œæ¶æ„
        # è¾“å…¥ï¼šå›¾åƒç‰¹å¾ + çŠ¶æ€å‘é‡
        # è¾“å‡ºï¼šåŠ¨ä½œï¼ˆthrottle, steer, brakeï¼‰

        # è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„å®é™…ç½‘ç»œç»“æ„è°ƒæ•´
        # ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨ä¸€ä¸ªç®€å•çš„CNN+MLP

        self.policy_network = SimplePolicyNetwork(
            image_channels=3,
            state_dim=6,
            action_dim=3,
            hidden_dims=self.config.get('hidden_dims', [512, 256])
        ).to(self.device)

        print(f"âœ… ç½‘ç»œå‚æ•°é‡: {self._count_parameters():,}")

    def _count_parameters(self):
        return sum(p.numel() for p in self.policy_network.parameters() if p.requires_grad)

    def train_epoch(self, dataloader, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.policy_network.train()

        epoch_loss = 0.0
        num_batches = 0

        pbar = tqdm(dataloader, desc=f"Epoch {epoch}")

        for batch_idx, batch in enumerate(pbar):
            # è·å–æ•°æ®
            images = batch['image'].to(self.device)       # (B, 3, H, W)
            states = batch['state'].to(self.device)       # (B, 6)
            actions = batch['action'].to(self.device)     # (B, 3)

            # å‰å‘ä¼ æ’­
            predicted_actions = self.policy_network(images, states)

            # è®¡ç®—æŸå¤±
            loss = self.criterion(predicted_actions, actions)

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            # ç»Ÿè®¡
            epoch_loss += loss.item()
            num_batches += 1

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'avg_loss': f'{epoch_loss/num_batches:.4f}'
            })

        avg_loss = epoch_loss / num_batches
        return avg_loss

    def train(self, train_loader, num_epochs):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("="*70)
        print("ğŸš€ å¼€å§‹è®­ç»ƒ")
        print("="*70)
        print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
        print(f"   - Epochs: {num_epochs}")
        print(f"   - Batch size: {train_loader.batch_size}")
        print(f"   - Learning rate: {self.config['learning_rate']}")
        print(f"   - è®¾å¤‡: {self.device}")
        print(f"   - æ•°æ®é›†å¤§å°: {len(train_loader.dataset)}")
        print("="*70)
        print()

        best_loss = float('inf')

        for epoch in range(1, num_epochs + 1):
            print(f"\nğŸ“… Epoch {epoch}/{num_epochs}")
            print("-"*70)

            # è®­ç»ƒä¸€ä¸ªepoch
            avg_loss = self.train_epoch(train_loader, epoch)

            # è®°å½•
            self.train_losses.append(avg_loss)

            # æ‰“å°ç»Ÿè®¡
            print(f"\nğŸ“Š Epoch {epoch} ç»Ÿè®¡:")
            print(f"   - å¹³å‡æŸå¤±: {avg_loss:.6f}")

            # ä¿å­˜checkpoint
            if epoch % self.config.get('save_interval', 5) == 0:
                self.save_checkpoint(epoch, avg_loss)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_loss < best_loss:
                best_loss = avg_loss
                self.save_checkpoint(epoch, avg_loss, is_best=True)
                print(f"   âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (loss: {best_loss:.6f})")

            print()

        print("="*70)
        print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“Š æœ€ä½³æŸå¤±: {best_loss:.6f}")
        print("="*70)

    def save_checkpoint(self, epoch, loss, is_best=False):
        """ä¿å­˜checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.policy_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'loss': loss,
            'config': self.config
        }

        if is_best:
            path = self.save_dir / 'best_model.pth'
        else:
            path = self.save_dir / f'checkpoint_epoch_{epoch}.pth'

        torch.save(checkpoint, path)
        print(f"   ğŸ’¾ ä¿å­˜: {path}")


class SimplePolicyNetwork(nn.Module):
    """
    ç®€åŒ–çš„ç­–ç•¥ç½‘ç»œ
    è¾“å…¥ï¼šå›¾åƒ + çŠ¶æ€å‘é‡
    è¾“å‡ºï¼šåŠ¨ä½œï¼ˆthrottle, steer, brakeï¼‰
    """

    def __init__(self, image_channels=3, state_dim=6, action_dim=3, hidden_dims=[512, 256]):
        super().__init__()

        # å›¾åƒç¼–ç å™¨ï¼ˆç®€å•çš„CNNï¼‰
        self.image_encoder = nn.Sequential(
            nn.Conv2d(image_channels, 32, 5, stride=2, padding=2),
            nn.ReLU(),
            nn.Conv2d(32, 64, 5, stride=2, padding=2),
            nn.ReLU(),
            nn.Conv2d(64, 128, 5, stride=2, padding=2),
            nn.ReLU(),
            nn.Conv2d(128, 256, 3, stride=2, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )

        # MLPå±‚
        self.fc = nn.Sequential(
            nn.Linear(256 + state_dim, hidden_dims[0]),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_dims[0], hidden_dims[1]),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_dims[1], action_dim),
            nn.Sigmoid()  # è¾“å‡ºåˆ°[0, 1]èŒƒå›´
        )

    def forward(self, image, state):
        # ç¼–ç å›¾åƒ
        img_features = self.image_encoder(image)
        img_features = img_features.view(img_features.size(0), -1)

        # æ‹¼æ¥å›¾åƒç‰¹å¾å’ŒçŠ¶æ€
        combined = torch.cat([img_features, state], dim=1)

        # é¢„æµ‹åŠ¨ä½œ
        action = self.fc(combined)

        # è°ƒæ•´steerçš„èŒƒå›´åˆ°[-1, 1]
        throttle = action[:, 0:1]
        steer = action[:, 1:2] * 2 - 1  # [0,1] -> [-1,1]
        brake = action[:, 2:3]

        return torch.cat([throttle, steer, brake], dim=1)


def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("ğŸ¯ B2D PPOè¡Œä¸ºå…‹éš†è®­ç»ƒ")
    print("="*70)
    print()

    # è®­ç»ƒé…ç½®
    config = {
        # æ•°æ®é…ç½®
        'data_root': '/home/ajifang/b2drive/Bench2Drive-RL50GB/datasets--rethinklab--Bench2Drive/snapshots',
        'image_size': (224, 224),
        'batch_size': 32,
        'num_workers': 4,
        'max_clips': None,  # None=ä½¿ç”¨å…¨éƒ¨clips

        # è®­ç»ƒé…ç½®
        'num_epochs': 50,
        'learning_rate': 3e-4,
        'hidden_dims': [512, 256],

        # ä¿å­˜é…ç½®
        'save_dir': '/home/ajifang/b2drive/rl_ppo_model/checkpoints',
        'save_interval': 5,
    }

    # æ‰¾åˆ°snapshotç›®å½•
    import glob
    snapshot_dirs = glob.glob(f"{config['data_root']}/*")
    if snapshot_dirs:
        config['data_root'] = snapshot_dirs[0]
        print(f"ğŸ“‚ æ•°æ®ç›®å½•: {config['data_root']}")
    else:
        print("âŒ æ‰¾ä¸åˆ°snapshotç›®å½•")
        exit(1)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\nğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader = create_dataloader(
        data_root=config['data_root'],
        batch_size=config['batch_size'],
        image_size=config['image_size'],
        num_workers=config['num_workers'],
        shuffle=True,
        max_clips=config['max_clips']
    )

    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
    print(f"   - è®­ç»ƒæ ·æœ¬æ•°: {len(train_loader.dataset)}")
    print(f"   - Batchæ•°é‡: {len(train_loader)}")
    print()

    # åˆ›å»ºè®­ç»ƒå™¨
    print("ğŸ—ï¸  åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = BehaviorCloningTrainer(config)
    print()

    # å¼€å§‹è®­ç»ƒ
    trainer.train(train_loader, config['num_epochs'])

    print("\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“‚ æ¨¡å‹ä¿å­˜åœ¨: {config['save_dir']}")


if __name__ == "__main__":
    main()
