<div id="top">

# Awesome-GE2EAD

<!-- PROJECT SHIELDS -->
[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]
[![MIT License][license-shield]][license-url]
[![LinkedIn][linkedin-shield]][linkedin-url]


This is the official repository for the paper **"Survey of General End-to-End Autonomous Driving: A Unified Perspective"** ([paper link](https://doi.org/10.36227/techrxiv.176523315.56439138/v1)).  
It collects and organizes key papers in General End-to-End Autonomous Driving, classifying them into **Conventional** (e.g., UniAD), **VLM-centric** (e.g., DriveLM), and **Hybrid** (e.g., Senna) approaches.  
In addition, this repository curates both **Normal** and **Vision-Language** datasets relevant to General End-to-End Autonomous Driving.  
Based on this taxonomy and dataset collection, our analysis further outlines the main research branches and emerging trends that are shaping the field.

## üìå Milestones

- üöÄ **2025-12-13**: Added the latest newly published papers to the survey repository and updated the overall organization.

- üöÄ **2025-12-10**: The paper *‚ÄúSurvey of General End-to-End Autonomous Driving: A Unified Perspective‚Äù* was released, and this repository was made publicly available.




## Table of Contents

- [Mindmap, Top Methods](#mindmap-top-methods)
- [Papers](#papers)
    - [Conventional End-to-End Methods](#conventional-end-to-end-methods)
    - [VLM-Centric End-to-End Methods](#vlm-centric-end-to-end-methods)
    - [Hybrid End-to-End Methods](#hybrid-end-to-end-methods)
- [Dataset](#dataset)
    - [Normal Dataset](#normal-dataset)
    - [Vision Language Dataset](#vision-language-dataset)
- [License](#license)
- [Citation](#citation)

## Mindmap, Top Methods

<div align="center">
  <a href="https://github.com/AutoLab-SAI-SJTU/GE2EAD">
    <img 
      src="images/coggle.png" 
      alt="GE2EAD Mindmap Logo" 
      style="width: 100%; height: auto; max-width: 1500px;" 
    >
  </a>
  <h3 align="center">GE2EAD Mindmap</h3>
</div>

<div align="center">
  <a href="https://github.com/AutoLab-SAI-SJTU/GE2EAD">
    <img 
      src="images/top_methods.png" 
      alt="GE2EAD Mindmap Logo" 
      style="width: 100%; height: auto; max-width: 1500px;" 
    >
  </a>
  <h3 align="center">Top Methods</h3>
</div>

## Papers
<details open>
<summary> Conventional End-to-End Methods </summary>

### Conventional End-to-End Methods


<details open>
<summary>2025</summary>

| üß† **Method** | üóìÔ∏è **Year / Venue** | üè∑Ô∏è **Tags** | üìÑ **Paper** | üíª **GitHub** | üåê **Project** |
|---|---|---|---|---|---|
| **DiffusionDriveV2**<br><sub>DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving ] ]</sub><br><sub><i>Summary:</i> DiffusionDriveV2 enhances diffusion-based trajectory planning by combining truncated diffusion modeling with reinforcement learning. It uses scale-adaptive multiplicative noise to promote broad exploration, and designs intra-anchor GRPO to optimize samples within the same intention anchor plus inter-anchor truncated GRPO to compare advantages across anchors in a controlled way, suppressing low-quality modes while preserving multimodal, intention-diverse trajectories.</sub> | ‚Äî | `Diffusion` ¬∑ `RL` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2512.07745-b31b1b?style=flat-square)](https://arxiv.org/abs/2512.07745) | [![Stars](https://img.shields.io/github/stars/hustvl/DiffusionDriveV2?style=social)](https://github.com/hustvl/DiffusionDriveV2) | ‚Äî |
| **SIMSCALE**<br><sub>SimScale: Learning to Drive via Real-World Simulation at Scale ] ]</sub><br><sub><i>Summary:</i> SimScale is a scalable real-world‚Äìconditioned simulation framework that augments driving logs with massive, diverse, safety-critical scenarios. It uses neural rendering with a reactive environment to generate multi-view observations along perturbed ego trajectories, then synthesizes pseudo-expert trajectories for those new states; policies are cotrained on both real and simulated data, yielding improved robustness and generalization that scale with simulation data volume.</sub> | ‚Äî | `Safety` | [![arXiv](https://img.shields.io/badge/arXiv-2511.23369-b31b1b?style=flat-square)](https://arxiv.org/abs/2511.23369) | [![Stars](https://img.shields.io/github/stars/OpenDriveLab/SimScale?style=social)](https://github.com/OpenDriveLab/SimScale) | ‚Äî |
| **LAP**<br><sub>LAP: Fast Latent Diffusion Planner with Fine-Grained Feature Distillation for Autonomous Driving ] ]</sub><br><sub><i>Summary:</i> LAP is a latent-space diffusion planner that decouples high-level driving intents from low-level kinematics using a VAE-learned latent representation. Trajectories are generated via diffusion in this latent space, while a fine-grained feature distillation mechanism tightly fuses the latent planner with vectorized scene context, enabling high-quality, multimodal plans in as few as a single denoising step and significantly reducing inference latency.</sub> | ‚Äî | `Diffusion` | [![arXiv](https://img.shields.io/badge/arXiv-2512.00470-b31b1b?style=flat-square)](https://arxiv.org/abs/2512.00470) | [![Stars](https://img.shields.io/github/stars/jhz1192/Latent-Planner?style=social)](https://github.com/jhz1192/Latent-Planner) | ‚Äî |
| **GuideFlow**<br><sub>GuideFlow: Constraint-Guided Flow Matching for Planning in End-to-End Autonomous Driving ] ]</sub><br><sub><i>Summary:</i> GuideFlow is a generative planner that uses constrained flow matching to generate diverse yet constraint-satisfying trajectories. It explicitly models the flow matching process, jointly trains with an Energy-Based Model to enforce physical and safety constraints during generation, and parameterizes driving aggressiveness as a control signal to modulate trajectory style.</sub> | ‚Äî | `Planning` ¬∑ `Safety` | [![arXiv](https://img.shields.io/badge/arXiv-2511.18729-b31b1b?style=flat-square)](https://arxiv.org/abs/2511.18729) | [![Stars](https://img.shields.io/github/stars/liulin815/GuideFlow?style=social)](https://github.com/liulin815/GuideFlow) | ‚Äî |
| **DiffRefiner**<br><sub>DiffRefiner: Coarse to Fine Trajectory Planning via Diffusion Refinement with Semantic Interaction for End to End Autonomous Driving ] ]</sub><br><sub><i>Summary:</i> DiffRefiner adopts a two-stage coarse-to-fine trajectory planning framework where a transformer-based proposal decoder first regresses coarse trajectories from sensor inputs using predefined anchors, and a diffusion refiner then iteratively denoises and refines these proposals. A fine-grained denoising decoder with semantic interaction improves scene compliance, yielding more accurate and environment-aligned trajectories.</sub> | ‚Äî | `Diffusion` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2511.17150-b31b1b?style=flat-square)](https://arxiv.org/abs/2511.17150) | [![Stars](https://img.shields.io/github/stars/nullmax-vision/DiffRefiner?style=social)](https://github.com/nullmax-vision/DiffRefiner) | ‚Äî |
| **ResAD**<br><sub>ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving ] ]</sub><br><sub><i>Summary:</i> ResAD reframes end-to-end trajectory prediction as normalized residual modeling around a deterministic inertial reference path. The model predicts deviations from this inertial prior instead of absolute trajectories and applies point-wise normalization to reweight residuals across time, reducing the dominance of uncertain long-horizon errors and focusing learning on necessary, context-driven corrections that improve near-term safety.</sub> | ‚Äî | `WorldModel` ¬∑ `Planning` ¬∑ `Safety` | [![arXiv](https://img.shields.io/badge/arXiv-2510.08562-b31b1b?style=flat-square)](https://arxiv.org/abs/2510.08562) | [![Stars](https://img.shields.io/github/stars/Duckyee728/ResAD-released?style=social)](https://github.com/Duckyee728/ResAD-released) | ‚Äî |
| **SeerDrive**<br><sub>Future-Aware End-to-End Driving: Bidirectional Modeling of Trajectory Planning and Scene Evolution, ] ]</sub><br><sub><i>Summary:</i> SeerDrive jointly models future scene evolution and trajectory planning in a closed-loop manner. It first predicts future BEV representations to anticipate scene dynamics, then injects these future-aware features into the trajectory planner, and iteratively refines both future scene prediction and trajectory generation through collaborative optimization.</sub> | 2025 / NeurIPS | `WorldModel` ¬∑ `Planning` ¬∑ `BEV` | [![arXiv](https://img.shields.io/badge/arXiv-2510.11092-b31b1b?style=flat-square)](https://arxiv.org/abs/2510.11092) | [![Stars](https://img.shields.io/github/stars/LogosRoboticsGroup/SeerDrive?style=social)](https://github.com/LogosRoboticsGroup/SeerDrive) | ‚Äî |
| **DriveDPO**<br><sub>DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving ]</sub><br><sub><i>Summary:</i> DriveDPO learns an end-to-end driving policy by directly aligning it with safety-aware preferences instead of pure imitation. It first distills a unified policy distribution that combines human imitation similarity and rule-based safety scores, then performs iterative Direct Preference Optimization at the trajectory level to align the policy with safer driving behaviors.</sub> | ‚Äî | `Planning` ¬∑ `Safety` ¬∑ `Imitation` | [![arXiv](https://img.shields.io/badge/arXiv-2509.17940-b31b1b?style=flat-square)](https://arxiv.org/abs/2509.17940) | ‚Äî | ‚Äî |
| **AnchDrive**<br><sub>AnchDrive: Bootstrapping Diffusion Policies with Hybrid Trajectory Anchors for End-to-End Driving ]</sub><br><sub><i>Summary:</i> AnchDrive accelerates diffusion-based planners by bootstrapping from hybrid trajectory anchors instead of pure noise. It constructs anchors from a static vocabulary of general driving priors and dynamic context-aware trajectories decoded from dense and sparse perception features, then uses a diffusion model to predict offset distributions that refine these anchors into diverse, high-quality trajectories.</sub> | ‚Äî | `Diffusion` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2509.20253-b31b1b?style=flat-square)](https://arxiv.org/abs/2509.20253) | ‚Äî | ‚Äî |
| **AdaThinkDrive**<br><sub>AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving ]</sub><br><sub><i>Summary:</i> AdaThinkDrive is an end-to-end framework with a dual-mode ‚Äúfast/slow thinking‚Äù mechanism that adaptively decides whether to use Chain-of-Thought reasoning. The model is pretrained on QA and trajectory data to acquire driving knowledge, then supervised with mixed fast-answering and CoT-style slow-thinking data, and finally optimized with an Adaptive Think Reward and GRPO to learn when CoT reasoning improves trajectory quality versus when it is unnecessary.</sub> | ‚Äî | `RL` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2509.13769-b31b1b?style=flat-square)](https://arxiv.org/abs/2509.13769) | ‚Äî | ‚Äî |
| **VeteranAD**<br><sub>Perception in Plan: Coupled Perception and Planning for End-to-End Autonomous Driving ] ]</sub><br><sub><i>Summary:</i> The method proposes a ‚Äúperception-in-plan‚Äù framework that tightly couples perception with planning by using multi-mode anchored trajectories as planning priors. The perception module is guided to collect traffic elements specifically along these priors, and an autoregressive planner progressively predicts future trajectories while repeatedly focusing perception on the most relevant regions, leading to targeted, planning-driven perception and trajectory generation.</sub> | ‚Äî | `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2508.11488-b31b1b?style=flat-square)](https://arxiv.org/abs/2508.11488) | [![Stars](https://img.shields.io/github/stars/LogosRoboticsGroup/VeteranAD?style=social)](https://github.com/LogosRoboticsGroup/VeteranAD) | ‚Äî |
* **EvaDrive**: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving [[Paper](https://www.arxiv.org/pdf/2508.09158)] 
    

    Summary: This paper integrates trajectory generation and evaluation into a closed-loop system through an innovative multi-objective reinforcement learning framework and adversarial strategy optimization, significantly enhancing the robustness and flexibility of autonomous driving planning. It also supports diverse driving styles and has broad application potential.
* **ReconDreamer-RL**: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction [[Paper](https://arxiv.org/pdf/2508.08170)] [[Code](https://github.com/GigaAI-research/ReconDreamer-RL)] 
    

    Summary: ReconDreamer-RL is a framework that enhances end-to-end reinforcement learning through scene reconstruction and video diffusion technologies, aiming to optimize the training performance of autonomous driving models, especially in handling complex scenes and corner cases in closed-loop environments.
* **GMF-Drive**: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving [[Paper](https://arxiv.org/pdf/2508.06113)] 
    

    Summary: GMF-Drive significantly enhances the efficiency of multimodal fusion and the performance of autonomous driving through innovative geometrically enhanced representations and spatially aware state-space models.
* **DistillDrive**: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model [[Paper](https://www.arxiv.org/pdf/2508.05402)] [[Code](https://github.com/YuruiAI/DistillDrive)] 
    

    Summary: This paper employs a multimodal decoupling planning model based on structured scene representation as the teacher model, guiding the student model to learn multimodal motion features through distillation, in order to address the limitations of existing end-to-end models in single-objective imitation learning.
* **GEMINUS**: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving [[Paper](https://www.arxiv.org/pdf/2507.14456)] [[Code](https://github.com/newbrains1/GEMINUS)] 
    

    Summary: GEMINUS effectively combines global experts with scene-adaptive expert groups through dual-sensing routers, achieving a balance between adaptability and robustness in complex and diverse traffic scenarios.
* **DiVER**: Breaking Imitation Bottlenecks: Reinforced Diffusion Powers Diverse Trajectory Generation [[Paper](https://arxiv.org/pdf/2507.04049)] 
    

    Summary: DiVER is an end-to-end driving framework that integrates reinforcement learning with diffusion-based generation to produce diverse and feasible trajectories, effectively addressing the mode collapse problem inherent in imitation learning.
* **World4Drive**: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model, **ICCV 2025** [[Paper](https://arxiv.org/pdf/2507.00603)] [[Project](https://github.com/ucaszyp/World4Drive)] 
    

    Summary: By simulating the evolution process of the physical world under different driving intentions, the generation and evaluation of multimodal trajectories are achieved, which is close to the decision-making logic of human drivers.
* **FocalAD**: Local Motion Planning for End-to-End Autonomous Driving [[Paper](https://arxiv.org/pdf/2506.11419)] 
    

    Summary: FocalAD refines planning by focusing on critical local neighbors and enhancing local motion representations.
* **GaussianFusion**: Gaussian-Based Multi-Sensor Fusion for End-to-End Autonomous Driving [[Paper](https://arxiv.org/pdf/2506.00034)] [[Code](https://github.com/Say2L/GaussianFusion)] 
    

    Summary: Utilizing intuitive and compact Gaussian representations as intermediate carriers, GaussianFusion iteratively refine trajectory predictions through interactions with the rich spatial and semantic information within these Gaussians.
* **CogAD**: Cognitive-Hierarchy Guided End-to-End Autonomous Driving [[Paper](https://arxiv.org/pdf/2505.21581)] 
    

    Summary: CogAD implements dual hierarchical mechanisms: global-to-local context processing for human-like perception and intent-conditioned multi-mode trajectory generation for cognitively-inspired planning.

* **DiffE2E**: Rethinking End-to-End Driving with a Hybrid Action Diffusion and Supervised Policy [[Paper](https://arxiv.org/pdf/2505.19516)] [[Project](https://infinidrive.github.io/DiffE2E/)] 
    

    Summary: DiffE2 integrates a Transformer-based hybrid diffusion-supervised decoder and introduces a collaborative training mechanism, which effectively combines the advantages of diffusion and supervision strategies.
* **TransDiffuser**: End-to-end Trajectory Generation with Decorrelated Multi-modal Representation for Autonomous Driving [[Paper](https://arxiv.org/pdf/2505.09315)] 
    

    Summary: TransDiffuser, an end-to-end generative trajectory model for autonomous driving based on "encoder-decoder", and introduces a multimodal representation decorrelation optimization mechanism to encourage sampling of more diverse trajectories from continuous space.
* **MomAD**: Don‚Äôt Shake the Wheel: Momentum-Aware Planning in End-to-End Autonomous Driving, **CVPR 2025** [[Paper](https://arxiv.org/pdf/2503.03125)] [[Code](https://github.com/adept-thu/MomAD)] 
    

    Summary: MomAD effectively alleviates the key challenges of trajectory mutation and perception instability in end-to-end autonomous driving through the momentum mechanism. Trajectory momentum aims to stabilize and optimize trajectory prediction by keeping candidate trajectories aligned with historical trajectories.
* **Consistency**: Predictive Planner for Autonomous Driving with Consistency Models [[Paper](https://arxiv.org/pdf/2502.08033)] 
    

    Summary: Consistency leverage the consistency model to build a predictive planner that samples from a joint distribution of ego and surrounding agents, conditioned on the ego vehicle‚Äôs navigational goal.
* **ARTEMIS**: Autoregressive End-to-End Trajectory Planning with Mixture of Experts for Autonomous Driving [[Paper](https://arxiv.org/abs/2504.19580)] 
    

    Summary: Using the hybrid expert model MoE to improve E2E, the autoregressive planning module with MOE gradually generates trajectory waypoints through a sequential decision process, while dynamically selecting the expert network that best suits the current driving scenario.
* **TTOG**: Two Tasks, One Goal: Uniting Motion and Planning for Excellent End To End Autonomous Driving Performance [[Paper](https://arxiv.org/pdf/2504.12667)] 
    

    Summary: TTOG introduces a new method to unify motion and planning tasks, allowing the planning task to benefit from motion data, significantly improving the performance and generalization ability of the planning task.
* **DiffusionDrive**: Truncated Diffusion Model for End-to-End Autonomous Driving, **CVPR 2025** [[Paper](https://arxiv.org/pdf/2411.15139)] [[Code](https://github.com/hustvl/DiffusionDrive)] 
    

    Summary: For the first time, the diffusion model was introduced into the field of end-to-end autonomous driving, and a truncated diffusion strategy was proposed, which solved the problems of mode collapse and excessive computation when the traditional diffusion strategy was applied in traffic scenarios.
* **WoTE**: End-to-End Driving with Online Trajectory Evaluation via BEV World Model [[Paper](https://arxiv.org/pdf/2504.01941)] [[Code](https://github.com/liyingyanUCAS/WoTE)] 
    

    Summary: The BEV world model is used to predict the future state of BEVs for trajectory evaluation. Compared with the image-level world model, the proposed BEV world model has lower latency and can be seamlessly supervised using an off-the-shelf BEV spatial traffic simulator.
* **DMAD**: Divide and Merge: Motion and Semantic Learning in End-to-End Autonomous Driving [[Paper](https://arxiv.org/pdf/2502.07631)] [[Code](https://github.com/shenyinzhe/DMAD)] 
    

    Summary: A novel parallel detection, tracking, and prediction method that separates semantic learning from motion learning. This architecture separates the gradient backpropagation between the two types of tasks to eliminate negative transfer, and merges similar tasks to exploit the correlation between tasks and promote positive transfer.
* **Centaur**: Robust End-to-End Autonomous Driving with Test-Time Training [[Paper](https://arxiv.org/abs/2503.11650)] 
    

    Summary: Application of **Test-Time Training (TTT)** in End-to-End Autonomous Driving to enhance robustness.
* **Drive in Corridors**: Enhancing the Safety of End-to-end Autonomous Driving via Corridor Learning and Planning [[Paper](https://arxiv.org/abs/2504.07507)] 
    

    Summary: The concept of a safe corridor in the field of robot planning is introduced into end-to-end autonomous driving as an explicit spatiotemporal constraint to enhance safety.
* **BridgeAD**: Bridging Past and Future: End-to-End Autonomous Driving with Historical Prediction and Planning, **CVPR 2025** [[Paper](https://arxiv.org/pdf/2503.14182v1)] [[Code](https://github.com/fudan-zvg/BridgeAD)] 
    

    Summary: End-to-end autonomous driving is enhanced by incorporating historical predictions of the current frame into the perception module, and incorporating historical predictions and planning of future frames into the motion planning module.
* **Hydra-MDP++**: Advancing End-to-End Driving via Expert-Guided Hydra-Distillation [[Paper](https://arxiv.org/pdf/2503.12820)] [[Code](https://github.com/NVlabs/Hydra-MDP)] 
    

    Summary: Hydra-MDP++ introduces a new teacher-student knowledge distillation framework with a multi-head decoder that can learn from human demonstrations and rule-based experts.
* **DiffAD**: A Unified Diffusion Modeling Approach for Autonomous Driving [[Paper](https://arxiv.org/pdf/2503.12170)] 
    

    Summary: DiffAD redefines autonomous driving as a conditional image generation task. By rasterizing heterogeneous targets onto a unified bird‚Äôs-eye view (BEV) and modeling their latent distribution, DiffAD unifies various driving objectives and jointly optimizes all driving tasks in a single framework.
* **GoalFlow**: Goal-Driven Flow Matching for Multimodal Trajectories Generation in End-to-End Autonomous Driving, **CVPR 2025** [[Paper](https://arxiv.org/pdf/2503.05689)] [[Code](https://github.com/YvanYin/GoalFlow)] 
    

    Summary: It has come up with an end-to-end autonomous driving method called GoalFlow, which generates high-quality multimodal trajectories by introducing target points to constrain the generation process using flow matching.
* **HiP-AD**: Hierarchical and Multi-Granularity Planning with Deformable Attention for Autonomous Driving in a Single Decoder, **ICCV 2025** [[Paper](https://arxiv.org/pdf/2503.08612)] [[Code](https://github.com/nullmax-vision/HiP-AD)] 
    

    Summary: A unified decoder is designed to take as input hybrid task queries (detection, map understanding, planning), allowing planning and perception tasks to exchange information in BEV space and planning queries to interact with image space.
* **LAW**: Enhancing End-to-End Autonomous Driving with Latent World Model, **ICLR 2025** [[Paper](https://arxiv.org/pdf/2406.08481)] [[Code](https://github.com/BraveGroup/LAW)] 
    

    Summary: This paper proposes a self-supervised learning method based on the LAtent World Model (LAW) to optimize the scene feature representation and future trajectory prediction.
* **DriveTransformer**: Unified Transformer for Scalable End-to-End Autonomous Driving, **ICLR 2025** [[Paper](https://arxiv.org/pdf/2503.07656)] [[Code](https://github.com/Thinklab-SJTU/DriveTransformer)] 
    

    Summary: A unified architecture without BEV is designed with a Decoder as the core, featuring task parallelism, sparse representation (task queries interact directly with raw sensor features), and stream processing.
* **UncAD**: Towards Safe End-to-end Autonomous Driving via Online Map Uncertainty, **ICRA 2025** [[Paper](https://arxiv.org/pdf/2504.12826)] [[Code](https://github.com/pengxuanyang/UncAD)] 
    

    Summary: UncAD effectively utilize the map uncertainty to produce robust and safe planning results via Uncertainty-Guided Planning strategy and Uncertainty-Collision-Aware Planning Selection module.
* **RAD**: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning [[Paper](https://arxiv.org/pdf/2502.13144)] [[Project](https://hgao-cv.github.io/RAD/)] 
    

    Summary: Establish a closed-loop reinforcement learning (RL) training paradigm based on 3DGS to build a realistic digital replica of the real physical world, enabling AD policies to explore the state space and handle out-of-distribution (OOD) scenarios.
* **OAD**: Trajectory Offset Learning: A Framework for Enhanced End-to-End Autonomous Driving [[Paper](https://www.researchgate.net/publication/388891609_Trajectory_Offset_Learning_A_Framework_for_Enhanced_End-to-End_Autonomous_Driving)] [[Code](https://github.com/wzn-cv/OAD)] 
    

    Summary: OAD enhanced framework built upon the VAD architecture, which introduces a novel paradigm shift from direct trajectory prediction to trajectory offset learning. Leverage trajectory vocabulary to learn offsets instead of learning trajectories directly.

</details>
<details open>
<summary>2024</summary>

| üß† **Method** | üóìÔ∏è **Year / Venue** | üè∑Ô∏è **Tags** | üìÑ **Paper** | üíª **GitHub** | üåê **Project** |
|---|---|---|---|---|---|
| **GaussianAD**<br><sub>Gaussian-Centric End-to-End Autonomous Driving ] ]</sub><br><sub><i>Summary:</i> The author extensively and sparsely describes the scene by using 3D semantic Gaussian, efficiently performs 3D perception with sparse convolution, uses Gaussian 3D flow prediction, and plans the self-vehicle trajectory accordingly with the goal of future scene prediction.</sub> | ‚Äî | `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2412.10371-b31b1b?style=flat-square)](https://arxiv.org/abs/2412.10371) | [![Stars](https://img.shields.io/github/stars/wzzheng/GaussianAD?style=social)](https://github.com/wzzheng/GaussianAD) | ‚Äî |
| **MA2T**<br><sub>Module-wise Adaptive Adversarial Training for End-to-end Autonomous Driving ]</sub><br><sub><i>Summary:</i> Adversative training is applied to end-to-end AD to improve the robustness under different adversative attacks by integrating module-level noise injection and dynamic weight accumulation adaptation.</sub> | ‚Äî | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2409.07321-b31b1b?style=flat-square)](https://arxiv.org/abs/2409.07321) | ‚Äî | ‚Äî |
| **Hint-AD**<br><sub>Holistically Aligned Interpretability in End-to-End Autonomous Driving ] ] ]</sub><br><sub><i>Summary:</i> By combining the intermediate output and the token mixer subnetwork, the language generated by the model is aligned with the overall perception-prediction-planning output of the AD model.</sub> | ‚Äî | `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2409.06702-b31b1b?style=flat-square)](https://arxiv.org/abs/2409.06702) | [![Stars](https://img.shields.io/github/stars/Robot-K/Hint-AD?style=social)](https://github.com/Robot-K/Hint-AD) | [Project](https://air-discover.github.io/Hint-AD/) |
| **DRAMA**<br><sub>An Efficient End-to-end Motion Planner for Autonomous Driving with Mamba, ] ] ]</sub><br><sub><i>Summary:</i> Using the Mamba-embedded encoder-decoder architecture, the encoder is used to fuse fuses features from the camera and LiDAR BEV images, and the decoder is used to generate motion trajectories.</sub> | 2025 / CVPR | `BEV` | [![arXiv](https://img.shields.io/badge/arXiv-2408.03601-b31b1b?style=flat-square)](https://arxiv.org/abs/2408.03601) | [![Stars](https://img.shields.io/github/stars/Chengran-Yuan/DRAMA?style=social)](https://github.com/Chengran-Yuan/DRAMA) | [Project](https://chengran-yuan.github.io/DRAMA/) |
| **PPAD**<br><sub>Iterative Interactions of Prediction and Planning for End-to-end Autonomous Driving, ] ]</sub><br><sub><i>Summary:</i> The prediction and planning processes are carried out alternately at each time step, rather than a single sequential process of prediction and planning.</sub> | 2024 / ECCV | `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2311.08100-b31b1b?style=flat-square)](https://arxiv.org/abs/2311.08100) | [![Stars](https://img.shields.io/github/stars/zlichen/PPAD?style=social)](https://github.com/zlichen/PPAD) | ‚Äî |
| **BEV-Planner**<br><sub>Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving?, ] ]</sub><br><sub><i>Summary:</i> This paper analyzes the problem of excessive reliance on the ego status in planning tasks by existing methods, proposes new baseline methods and evaluation metrics, and emphasizes the importance of developing more suitable datasets.</sub> | 2024 / CVPR | `Dataset` ¬∑ `Planning` ¬∑ `BEV` | [![arXiv](https://img.shields.io/badge/arXiv-2312.03031-b31b1b?style=flat-square)](https://arxiv.org/abs/2312.03031) | [![Stars](https://img.shields.io/github/stars/NVlabs/BEV-Planner?style=social)](https://github.com/NVlabs/BEV-Planner) | ‚Äî |
| **EfficientFuser**<br><sub>Efficient Fusion and Task Guided Embedding for End-to-end Autonomous Driving ]</sub><br><sub><i>Summary:</i> Reduce required parameters and computation with EfficientViT lightweight neural networks.</sub> | ‚Äî | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2407.02878-b31b1b?style=flat-square)](https://arxiv.org/abs/2407.02878) | ‚Äî | ‚Äî |
| **UAD**<br><sub>End-to-End Autonomous Driving without Costly Modularization and 3D Manual Annotation ]</sub><br><sub><i>Summary:</i> Using unsupervised frameworks eliminate the need for expensive 3D annotation and use self-supervised training strategies to enhance the planning robustness in the transition scene.</sub> | ‚Äî | `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2406.17680-b31b1b?style=flat-square)](https://arxiv.org/abs/2406.17680) | ‚Äî | ‚Äî |
| **Hydra-MDP**<br><sub>End-to-end Multimodal Planning with Multi-target Hydra-Distillation ] ]</sub><br><sub><i>Summary:</i> The student model learns diverse trajectory candidates tailored for various evaluation metrics through the knowledge distillation of human teachers and rule-based teachers.</sub> | ‚Äî | `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2406.06978-b31b1b?style=flat-square)](https://arxiv.org/abs/2406.06978) | [![Stars](https://img.shields.io/github/stars/NVlabs/Hydra-MDP?style=social)](https://github.com/NVlabs/Hydra-MDP) | ‚Äî |
| **DualAD**<br><sub>Disentangling the Dynamic and Static World for End-to-End Driving, ] ]</sub><br><sub><i>Summary:</i> A dual-stream architecture that decouples dynamic agents and static scene elements is used to compensate for the movement of self-vehicles and objects, enabling the system to better integrate information in the time dimension.</sub> | 2025 / CVPR | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2406.06264-b31b1b?style=flat-square)](https://arxiv.org/abs/2406.06264) | [![Stars](https://img.shields.io/github/stars/TUM-AVS/DualAD?style=social)](https://github.com/TUM-AVS/DualAD) | ‚Äî |
| **SparseDrive**<br><sub>End-to-End Autonomous Driving via Sparse Scene Representation ] ]</sub><br><sub><i>Summary:</i> Sparse scene representation is used, while adding a parallel motion planner and using a hierarchical programming selection strategy to improve the performance of the model.</sub> | ‚Äî | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2405.19620-b31b1b?style=flat-square)](https://arxiv.org/abs/2405.19620) | [![Stars](https://img.shields.io/github/stars/swc-17/SparseDrive?style=social)](https://github.com/swc-17/SparseDrive) | ‚Äî |
| **GAD**<br><sub>GAD-Generative Learning for HD Map-Free Autonomous Driving ] ]</sub><br><sub><i>Summary:</i> The data-driven predictive - planning framework without high-precision maps goes beyond the planning methods of simple imitation or trajectory sampling.</sub> | ‚Äî | `Planning` ¬∑ `Imitation` | [![arXiv](https://img.shields.io/badge/arXiv-2405.00515-b31b1b?style=flat-square)](https://arxiv.org/abs/2405.00515) | [![Stars](https://img.shields.io/github/stars/mr-d-self-driving/GAD?style=social)](https://github.com/mr-d-self-driving/GAD) | ‚Äî |
| **SparseAD**<br><sub>Sparse Query-Centric Paradigm for Efficient End-to-End Autonomous Driving ]</sub><br><sub><i>Summary:</i> Using the sparse query center paradigm to reduce computing costs and memory usage enables the utilization of longer historical information.</sub> | ‚Äî | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2404.06892-b31b1b?style=flat-square)](https://arxiv.org/abs/2404.06892) | ‚Äî | ‚Äî |
| **GenAD**<br><sub>Generative End-to-End Autonomous Driving, ] ]</sub><br><sub><i>Summary:</i> Use a unified future trajectory generation model to perform motion prediction and planning simultaneously for introducing trajectory priors and higher-order interactions.</sub> | 2024 / ECCV | `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2402.11502-b31b1b?style=flat-square)](https://arxiv.org/abs/2402.11502) | [![Stars](https://img.shields.io/github/stars/wzzheng/GenAD?style=social)](https://github.com/wzzheng/GenAD) | ‚Äî |
| **GraphAD**<br><sub>Interaction Scene Graph for End-to-end Autonomous Driving ] ]</sub><br><sub><i>Summary:</i> The graph model is adopted to describe the complex interactions in the traffic scene and introduce powerful prior knowledge.</sub> | ‚Äî | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2403.19098-b31b1b?style=flat-square)](https://arxiv.org/abs/2403.19098) | [![Stars](https://img.shields.io/github/stars/zhangyp15/GraphAD?style=social)](https://github.com/zhangyp15/GraphAD) | ‚Äî |
| **ActiveAD**<br><sub>Planning-Oriented Active Learning for End-to-End Autonomous Driving ]</sub><br><sub><i>Summary:</i> Utilize the active learning method oriented to planning to intelligently select the data that most needs annotation and improve the efficiency of data utilization.</sub> | ‚Äî | `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2403.02877-b31b1b?style=flat-square)](https://arxiv.org/abs/2403.02877) | ‚Äî | ‚Äî |
| **VADv2**<br><sub>End-to-End Vectorized Autonomous Driving via Probabilistic Planning ] ]</sub><br><sub><i>Summary:</i> Output the probability distribution of the action to deal with the uncertainty of the planning.</sub> | ‚Äî | `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2402.13243-b31b1b?style=flat-square)](https://arxiv.org/abs/2402.13243) | [![Stars](https://img.shields.io/github/stars/hustvl/vad?style=social)](https://github.com/hustvl/vad) | ‚Äî |
</details>

<details open>
<summary>2023</summary>

| üß† **Method** | üóìÔ∏è **Year / Venue** | üè∑Ô∏è **Tags** | üìÑ **Paper** | üíª **GitHub** | üåê **Project** |
|---|---|---|---|---|---|
| **DriveAdapter**<br><sub>Breaking the Coupling Barrier of Perception and Planning in End-to-End Autonomous Driving, ] ]</sub><br><sub><i>Summary:</i> DriveAdapter decouples the perceptual learning of the student model from the planning knowledge of the teacher model by introducing an adapter module, avoiding the causal confusion problem in traditional behavior cloning methods and improving the efficiency and performance of the autonomous driving system.</sub> | 2023 / ICCV | `Planning` ¬∑ `Imitation` | [![arXiv](https://img.shields.io/badge/arXiv-2308.00398-b31b1b?style=flat-square)](https://arxiv.org/abs/2308.00398) | [![Stars](https://img.shields.io/github/stars/OpenDriveLab/DriveAdapter?style=social)](https://github.com/OpenDriveLab/DriveAdapter) | ‚Äî |
| **VAD**<br><sub>Vectorized Scene Representation for Efficient Autonomous Driving, ] ]</sub><br><sub><i>Summary:</i> The design of vectorized environmental representation improves the processing speed, and instance-level planning constraints enhance planning security.</sub> | 2023 / ICCV | `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2303.12077-b31b1b?style=flat-square)](https://arxiv.org/abs/2303.12077) | [![Stars](https://img.shields.io/github/stars/hustvl/VAD?style=social)](https://github.com/hustvl/VAD) | ‚Äî |
| **ThinkTwice**<br><sub>Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving, ] ]</sub><br><sub><i>Summary:</i> This paper refines the prediction results layer by layer through an extensible decoder layer, combining spatio-temporal prior knowledge and intensive supervision, which enhances driving safety and task completion rate, and also provides new ideas for planner design.</sub> | 2023 / CVPR | `WorldModel` ¬∑ `Safety` | [![arXiv](https://img.shields.io/badge/arXiv-2305.06242-b31b1b?style=flat-square)](https://arxiv.org/abs/2305.06242) | [![Stars](https://img.shields.io/github/stars/OpenDriveLab/ThinkTwice?style=social)](https://github.com/OpenDriveLab/ThinkTwice) | ‚Äî |
| **ReasonNet**<br><sub>End-to-End Driving with Temporal and Global Reasoning, ] ]</sub><br><sub><i>Summary:</i> Use temporal reasoning module to effectively fuse information from different frames and a transformer-based global reasoning module for better scene understanding. Release a dataset DOS, which consists of diverse occlusion scenarios in urban driving for systematic evaluation of occlusion events.</sub> | 2023 / CVPR | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-2305.10507-b31b1b?style=flat-square)](https://arxiv.org/abs/2305.10507) | [![Stars](https://img.shields.io/github/stars/opendilab/DOS?tab=readme-ov-file?style=social)](https://github.com/opendilab/DOS?tab=readme-ov-file) | ‚Äî |
| **SuperDriverAI**<br><sub>Towards Design and Implementation for End-to-End Learning-based Autonomous Driving ]</sub><br><sub><i>Summary:</i> Employing simple DNN network to predict steering angle, and the visual attention module improves interpretability.</sub> | ‚Äî | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2305.10443-b31b1b?style=flat-square)](https://arxiv.org/abs/2305.10443) | ‚Äî | ‚Äî |
| **UniAD**<br><sub>Planning-oriented Autonomous Driving, ] ]</sub><br><sub><i>Summary:</i> The full-stack driving task is integrated in a query-based network. Different modules achieve feature complementarity and a global perspective, oriented towards the final planning task.</sub> | 2023 / CVPR | `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2212.10156-b31b1b?style=flat-square)](https://arxiv.org/abs/2212.10156) | [GitHub](https://arxiv.org/pdf/2212.10156) | ‚Äî |
| **End-to-End Learning of Behavioural Inputs for Autonomous Driving in Dense Traffic**<br><sub>End-to-End Learning of Behavioural Inputs for Autonomous Driving in Dense Traffic, ]</sub><br><sub><i>Summary:</i> By embedding a novel differentiable trajectory optimizer as the neural network layer, this method can dynamically adjust the behavioral input while ensuring the rapid convergence of the optimizer, thereby improving driving efficiency and reducing the collision rate.</sub> | 2023 / IROS | `Planning` ¬∑ `Safety` | [![arXiv](https://img.shields.io/badge/arXiv-2310.14766-b31b1b?style=flat-square)](https://arxiv.org/abs/2310.14766) | ‚Äî | ‚Äî |
| **CRCHFL**<br><sub>Communication Resources Constrained Hierarchical Federated Learning for End-to-End Autonomous Driving, ]</sub><br><sub><i>Summary:</i> This paper proposes an optimization-driven communication resource-constrained hierarchical federated learning framework (CRCHFL), aiming to address the trade-off between limited communication resources and learning performance in end-to-end autonomous driving scenarios.</sub> | 2023 / IROS | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2306.16169-b31b1b?style=flat-square)](https://arxiv.org/abs/2306.16169) | ‚Äî | ‚Äî |
| **PPGeo**<br><sub>Policy pre-training for autonomous driving via self-supervised geometric modeling, ] ]</sub><br><sub><i>Summary:</i> By performing self-supervised geometric modeling (pose/depth prediction and future ego-motion prediction) in stages on a large number of unlabeled YouTube driving videos, PPGeo pre-trained an encoder that can extract rich visual representations relevant to driving policies, thereby significantly improving the performance of visuo-motor driving tasks in data-constrained situations.</sub> | 2023 / ICLR | `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2301.01006-b31b1b?style=flat-square)](https://arxiv.org/abs/2301.01006) | [![Stars](https://img.shields.io/github/stars/OpenDriveLab/PPGeo?style=social)](https://github.com/OpenDriveLab/PPGeo) | ‚Äî |
</details>

<details open>
<summary>Before 2023</summary>

| üß† **Method** | üóìÔ∏è **Year / Venue** | üè∑Ô∏è **Tags** | üìÑ **Paper** | üíª **GitHub** | üåê **Project** |
|---|---|---|---|---|---|
| **MMFN**<br><sub>Multi-Modal-Fusion-Net for End-to-End Driving, ] ]</sub><br><sub><i>Summary:</i> This paper improves the driving performance of the autonomous driving model in complex urban environments by integrating camera, LiDAR, high-definition Map (HD Map) and radar data.</sub> | 2022 / IROS | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2207.00186-b31b1b?style=flat-square)](https://arxiv.org/abs/2207.00186) | [![Stars](https://img.shields.io/github/stars/Kin-Zhang/mmfn?style=social)](https://github.com/Kin-Zhang/mmfn) | ‚Äî |
| **KEMP**<br><sub>Keyframe-Based Hierarchical End-to-End Deep Model for Long-Term Trajectory Prediction, ]</sub><br><sub><i>Summary:</i> This paper predicts keyframes based on road context, and then fills in the intermediate states according to the keyframes and road context to generate complete trajectories, achieving the most advanced prediction performance.</sub> | 2022 / ICRA | `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2205.04624-b31b1b?style=flat-square)](https://arxiv.org/abs/2205.04624) | ‚Äî | ‚Äî |
| **TCP**<br><sub>Trajectory-guided Control Prediction for End-to-end Autonomous Driving: A Simple yet Strong Baseline, ] ]</sub><br><sub><i>Summary:</i> This paper combines two mainstream prediction paradigms: trajectory planning and direct control. By designing a unified learning framework and interaction mechanism, it fully leverages the advantages of both and optimizes the final output through context fusion strategies.</sub> | 2022 / NeurIPS | `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2206.08129-b31b1b?style=flat-square)](https://arxiv.org/abs/2206.08129) | [![Stars](https://img.shields.io/github/stars/OpenPerceptionX/TCP?style=social)](https://github.com/OpenPerceptionX/TCP) | ‚Äî |
| **ST-P3**<br><sub>End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning, ] ]</sub><br><sub><i>Summary:</i> ST-P3 proposes an interpretable end-to-end visual autonomous driving system, which realizes the joint spatiotemporal feature learning of perception, prediction and planning tasks by integrating self-centered alignment accumulation, dual-path prediction and time refinement units.</sub> | 2022 / ECCV | `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2207.07601-b31b1b?style=flat-square)](https://arxiv.org/abs/2207.07601) | [![Stars](https://img.shields.io/github/stars/OpenDriveLab/ST-P3?style=social)](https://github.com/OpenDriveLab/ST-P3) | ‚Äî |
| **MP3**<br><sub>A Unified Model to Map, Perceive, Predict and Plan, ]</sub><br><sub><i>Summary:</i> By online predicting the map and dynamic agent status and using this information for motion planning, it achieves driving performance that is safer, more comfortable and has higher command compliance than existing methods without the need for high-precision maps.</sub> | 2021 / CVPR | `Planning` ¬∑ `Safety` | [Paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Casas_MP3_A_Unified_Model_To_Map_Perceive_Predict_and_Plan_CVPR_2021_paper.pdf) | ‚Äî | ‚Äî |
| **Multitask-with-attention**<br><sub>Multi-task Learning with Attention for End-to-end Autonomous Driving, ] ]</sub><br><sub><i>Summary:</i> By integrating the Conditional Imitation Learning (CIL) framework and multi-task learning, the authors designed an attention-aware network to enhance the model's generalization ability and processing capacity for traffic signals.</sub> | 2021 / CVPR | `Imitation` | [![arXiv](https://img.shields.io/badge/arXiv-2104.10753-b31b1b?style=flat-square)](https://arxiv.org/abs/2104.10753) | [![Stars](https://img.shields.io/github/stars/KeishiIshihara/multitask-with-attention?style=social)](https://github.com/KeishiIshihara/multitask-with-attention) | ‚Äî |
| **Transfuser**<br><sub>Multi-Modal Fusion Transformer for End-to-End Autonomous Driving, ] ]</sub><br><sub><i>Summary:</i> TransFuser proposed a multimodal fusion Transformer based on the attention mechanism to integrate image and LiDAR data, thereby achieving a lower collision rate and better driving performance.</sub> | 2021 / CVPR | `Safety` | [Paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Prakash_Multi-Modal_Fusion_Transformer_for_End-to-End_Autonomous_Driving_CVPR_2021_paper.pdf) | [![Stars](https://img.shields.io/github/stars/autonomousvision/transfuser?style=social)](https://github.com/autonomousvision/transfuser) | ‚Äî |
| **NEAT**<br><sub>Neural Attention Fields for End-to-End Autonomous Driving, ] ]</sub><br><sub><i>Summary:</i> By learning the attention map from camera input to the spatio-temporal query location of BEV, the correlation problem of existing methods in BEV semantic prediction from camera input has been overcome, and robust and interpretable cloning of autonomous driving behavior has been achieved.</sub> | 2021 / ICCV | `WorldModel` ¬∑ `BEV` | [Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Chitta_NEAT_Neural_Attention_Fields_for_End-to-End_Autonomous_Driving_ICCV_2021_paper.pdf) | [![Stars](https://img.shields.io/github/stars/autonomousvision/neat?style=social)](https://github.com/autonomousvision/neat) | ‚Äî |
| **Fast-LiDARNet**<br><sub>Efficient and Robust LiDAR-Based End-to-End Navigation, ]</sub><br><sub><i>Summary:</i> This paper addresses the issues of high computational cost and insufficient model robustness in processing LiDAR data in existing methods, and enhances efficiency and reliability by optimizing the neural network structure and fusion algorithm.</sub> | 2021 / ICRA | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2105.09932-b31b1b?style=flat-square)](https://arxiv.org/abs/2105.09932) | ‚Äî | ‚Äî |
| **IVMP**<br><sub>Learning Interpretable End-to-End Vision-Based Motion Planning for Autonomous Driving with Optical Flow Distillation, ] ]</sub><br><sub><i>Summary:</i> This paper achieves interpretable trajectory planning by predicting future bird's-eye view semantic maps, and simultaneously adopts optical flow distillation technology to enhance network performance and maintain real-time performance.</sub> | 2021 / ICRA | `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2104.12861-b31b1b?style=flat-square)](https://arxiv.org/abs/2104.12861) | ‚Äî | [Project](https://sites.google.com/view/ivmp) |
| **P3**<br><sub>Perceive, Predict, and Plan: Safe Motion Planning Through Interpretable Semantic Representations, ]</sub><br><sub><i>Summary:</i> A novel end-to-end autonomous driving system based on the differentiable probabilistic semantic occupancy layer is proposed, aiming to solve the problems of information loss in traditional perception modules and inconsistency among modules in multi-task learning.</sub> | 2020 / ECCV | `Planning` ¬∑ `Safety` | [![arXiv](https://img.shields.io/badge/arXiv-2008.05930-b31b1b?style=flat-square)](https://arxiv.org/abs/2008.05930) | ‚Äî | ‚Äî |
| **DARB**<br><sub>Exploring data aggregation in policy learning for vision-based urban autonomous driving, ] ]</sub><br><sub><i>Summary:</i> A novel data aggregation method is proposed. By sampling key states and using replay buffers that focus on uncertainties, the generalization ability and robustness of visual driving strategies in complex traffic scenarios are significantly improved.</sub> | 2020 / CVPR | `Planning` | [Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Prakash_Exploring_Data_Aggregation_in_Policy_Learning_for_Vision-Based_Urban_Autonomous_CVPR_2020_paper.pdf) | [![Stars](https://img.shields.io/github/stars/autonomousvision/data_aggregation?style=social)](https://github.com/autonomousvision/data_aggregation) | ‚Äî |
| **Roach**<br><sub>End-to-End Urban Driving by Imitating a Reinforcement Learning Coach, ] ]</sub><br><sub><i>Summary:</i> This paper proposes an end-to-end imitation learning framework based on the guidance of reinforcement learning experts, which effectively improves the performance of autonomous driving agents through intensive supervision signals and multi-task learning objectives.</sub> | 2021 / ICCV | `RL` ¬∑ `Imitation` | [![arXiv](https://img.shields.io/badge/arXiv-2108.08265-b31b1b?style=flat-square)](https://arxiv.org/abs/2108.08265) | [![Stars](https://img.shields.io/github/stars/zhejz/carla-roach?style=social)](https://github.com/zhejz/carla-roach) | ‚Äî |
| **LBC**<br><sub>Learning by cheating, ] ]</sub><br><sub><i>Summary:</i> By using "privileged" agents as teachers to train pure visual perception autonomous driving systems, the driving performance in complex urban environments has been significantly improved.</sub> | 2019 / CoRL | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-1912.12294-b31b1b?style=flat-square)](https://arxiv.org/abs/1912.12294) | [![Stars](https://img.shields.io/github/stars/dotchen/LearningByCheating?style=social)](https://github.com/dotchen/LearningByCheating) | ‚Äî |
| **CIL**<br><sub>End-to-End driving via conditional imitation learning, ] ]</sub><br><sub><i>Summary:</i> The conditional imitation learning method based on advanced commands enables the deep learning system that imitates human driving to respond to specific navigation instructions during the test stage while processing sensor inputs.</sub> | 2018 / CoRL | `Imitation` | [![arXiv](https://img.shields.io/badge/arXiv-1710.02410-b31b1b?style=flat-square)](https://arxiv.org/abs/1710.02410) | [![Stars](https://img.shields.io/github/stars/carla-simulator/imitation-learning?style=social)](https://github.com/carla-simulator/imitation-learning) | ‚Äî |
| **Drive in A Day**<br><sub>Learning to drive in a day ] ]</sub><br><sub><i>Summary:</i> Deep reinforcement learning is applied to autonomous driving. Through autonomous exploration of vehicles and universal driving distance rewards, lane following can be learned merely based on monocular images.</sub> | ‚Äî | `RL` | [![arXiv](https://img.shields.io/badge/arXiv-1807.00412-b31b1b?style=flat-square)](https://arxiv.org/abs/1807.00412) | [![Stars](https://img.shields.io/github/stars/sheelabhadra/Learning2Drive?style=social)](https://github.com/sheelabhadra/Learning2Drive) | ‚Äî |
| **CNN E2E**<br><sub>End to End Learning for Self-Driving Cars ] ]</sub><br><sub><i>Summary:</i> By training an end-to-end CNN to directly convert the original pixels of the camera into steering instructions, autonomous driving in various complex road conditions was achieved, and its potential advantages in performance and system scale compared with the traditional step-by-step method were proved.</sub> | ‚Äî | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-1604.07316-b31b1b?style=flat-square)](https://arxiv.org/abs/1604.07316) | [![Stars](https://img.shields.io/github/stars/Nuclearstar/End-to-End-Learning-for-Self-Driving-Cars?style=social)](https://github.com/Nuclearstar/End-to-End-Learning-for-Self-Driving-Cars) | ‚Äî |
| **Alvinn**<br><sub>An autonomous land vehicle in a neural network, ]</sub><br><sub><i>Summary:</i> ALVINN is a three-layer backpropagation neural network. Through the input of cameras and laser rangefinders, it learns and outputs the direction in which vehicles travel along the road, demonstrating the adaptive potential to adjust processing capabilities according to different conditions.</sub> | NeurIPS 1988 | ‚Äî | [Paper](https://proceedings.neurips.cc/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf) | ‚Äî | ‚Äî |
</details>


<p align="right">(<a href="#top">back to top</a>)</p>    
</details>

<details open>
<summary>VLM-Centric End-to-End Methods</summary>


### VLM-Centric End-to-End Methods

<details open>
<summary>2025</summary>

| üß† **Method** | üóìÔ∏è **Year / Venue** | üè∑Ô∏è **Tags** | üìÑ **Paper** | üíª **GitHub** | üåê **Project** |
|---|---|---|---|---|---|
| **CoT4AD**<br><sub>CoT4AD: A Vision-Language-Action Model with Explicit Chain-of-Thought Reasoning for Autonomous Driving ]</sub><br><sub><i>Summary:</i> CoT4AD is a VLA framework that injects explicit Chain-of-Thought reasoning into autonomous driving. It takes visual observations and language instructions, then models an explicit perception‚Äìquestion‚Äìprediction‚Äìaction CoT during training to align the reasoning trajectory with the action space across multiple tasks; at inference, it performs implicit CoT reasoning to improve numerical reasoning, causal understanding, and trajectory planning in complex scenes.</sub> | ‚Äî | `VLM` ¬∑ `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2511.22532-b31b1b?style=flat-square)](https://arxiv.org/abs/2511.22532) | ‚Äî | ‚Äî |
| **Model-Based Policy Adaptation (MPA)**<br><sub>Model-Based Policy Adaptation for Closed-Loop End-to-End Autonomous Driving, ] ]</sub><br><sub><i>Summary:</i> MPA is a model-based adaptation framework that improves pretrained end-to-end policies at deployment time. It first uses a geometry-consistent simulation engine to generate diverse counterfactual trajectories, then trains a diffusion-based policy adapter to refine the base policy and a multi-step Q-value model to evaluate long-term outcomes; at inference, the adapter proposes multiple trajectory candidates and the Q model selects the one with highest expected utility for safer, more robust closed-loop behavior.</sub> | 2025 / NeurIPS | `Diffusion` ¬∑ `Planning` ¬∑ `Safety` | [![arXiv](https://img.shields.io/badge/arXiv-2511.21584-b31b1b?style=flat-square)](https://arxiv.org/abs/2511.21584) | ‚Äî | [Project](https://mpa-drive.github.io/) |
| **AD-R1**<br><sub>AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models ]</sub><br><sub><i>Summary:</i> AD-R1 introduces an Impartial World Model and a closed-loop RL refinement framework to overcome optimistic bias in world-model-based driving agents. It uses Counterfactual Synthesis to generate plausible failure cases (collisions, off-road events) so the world model learns to ‚Äúimagine danger‚Äù faithfully, then employs this world model as an internal critic that is queried to predict outcomes of candidate actions, enabling policy refinement that explicitly minimizes safety violations in challenging scenarios.</sub> | ‚Äî | `RL` ¬∑ `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2511.20325-b31b1b?style=flat-square)](https://arxiv.org/abs/2511.20325) | ‚Äî | ‚Äî |
| **Alpamayo-R1 (AR1)**<br><sub>Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail ] ]</sub><br><sub><i>Summary:</i> Alpamayo-R1 is a VLA framework that couples causal reasoning with trajectory planning using a modular architecture. It uses Cosmos-Reason, a vision‚Äìlanguage model pretrained for physical reasoning, to generate Chain-of-Causation traces aligned with driving decisions, and a diffusion-based trajectory decoder to produce feasible plans; a multi-stage training pipeline with supervised fine-tuning and RL further aligns reasoning quality and action consistency using LRM feedback.</sub> | ‚Äî | `Diffusion` ¬∑ `RL` ¬∑ `WorldModel` | [![arXiv](https://img.shields.io/badge/arXiv-2511.00088-b31b1b?style=flat-square)](https://arxiv.org/abs/2511.00088) | [![Stars](https://img.shields.io/github/stars/NVlabs/alpamayo?style=social)](https://github.com/NVlabs/alpamayo) | ‚Äî |
| **DriveVLA-W0**<br><sub>DRIVEVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving ] ]</sub><br><sub><i>Summary:</i> DriveVLA-W0 is a training paradigm for VLA models that augments sparse action supervision with dense world modeling signals by predicting future images. It instantiates both an autoregressive world model for discrete visual tokens and a diffusion world model for continuous visual features, and then adds a lightweight action expert on top of the learned representations to realize low-latency control for real-time deployment.</sub> | ‚Äî | `Diffusion` ¬∑ `WorldModel` | [![arXiv](https://img.shields.io/badge/arXiv-2510.12796-b31b1b?style=flat-square)](https://arxiv.org/abs/2510.12796) | [![Stars](https://img.shields.io/github/stars/BraveGroup/DriveVLA-W0?style=social)](https://github.com/BraveGroup/DriveVLA-W0) | ‚Äî |
| **MTRDrive**<br><sub>MTRDrive: Memory-Tool Synergistic Reasoning for Robust Autonomous Driving in Corner Cases ]</sub><br><sub><i>Summary:</i> MTRDrive builds a VLM-based end-to-end driving framework that augments vision-language reasoning with external memory and tools. It retrieves procedural driving experiences via a memory module, dynamically invokes toolkits in a closed-loop system, and combines these through memory‚Äìtool synergistic reasoning to improve robustness and generalization, especially in out-of-distribution and roadwork scenarios.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2509.20843-b31b1b?style=flat-square)](https://arxiv.org/abs/2509.20843) | ‚Äî | ‚Äî |
| **ReflectDrive**<br><sub>Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving ]</sub><br><sub><i>Summary:</i> ReflectDrive integrates a discrete diffusion-based reflection mechanism into VLA models for safe trajectory generation. It discretizes the driving space into an action codebook to adapt pre-trained diffusion language models for planning, generates goal-conditioned trajectories, and iteratively performs safety-aware reflection by detecting unsafe tokens and inpainting around safe anchors, enabling self-correction without gradient-based guidance.</sub> | ‚Äî | `Diffusion` ¬∑ `VLM` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2509.20109-b31b1b?style=flat-square)](https://arxiv.org/abs/2509.20109) | ‚Äî | ‚Äî |
| **IRL-VLA**<br><sub>IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model for End-to-End Autonomous Driving ] ]</sub><br><sub><i>Summary:</i> IRL-VLA is a closed-loop VLA framework trained via a reward world model built with inverse reinforcement learning. It first pretrains a VLA driving policy via imitation learning, then constructs a lightweight reward world model to provide efficient reward signals in closed-loop, and finally optimizes the VLA policy with PPO guided by this model to balance safety incidents, driving comfort, and traffic efficiency.</sub> | ‚Äî | `RL` ¬∑ `VLM` ¬∑ `WorldModel` | [![arXiv](https://img.shields.io/badge/arXiv-2508.06571-b31b1b?style=flat-square)](https://arxiv.org/abs/2508.06571) | [![Stars](https://img.shields.io/github/stars/IRL-VLA/IRL-VLA?style=social)](http://github.com/IRL-VLA/IRL-VLA) | ‚Äî |
| **Prune2Drive**<br><sub>Prune2Drive: A Plug-and-Play Framework for Accelerating Vision-Language Models in Autonomous Driving ]</sub><br><sub><i>Summary:</i> Prune2Drive accelerates multi-view VLMs for autonomous driving via plug-and-play visual token pruning that neither requires retraining nor attention maps. It uses a diversity-aware token selection mechanism inspired by farthest point sampling to retain tokens with broad semantic and spatial coverage, and a view-adaptive controller that learns distinct pruning ratios per camera view, greatly reducing compute while preserving vision-language reasoning for driving tasks.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2508.13305-b31b1b?style=flat-square)](https://arxiv.org/abs/2508.13305) | ‚Äî | ‚Äî |
| **FastDriveVLA**<br><sub>FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning ]</sub><br><sub><i>Summary:</i> This work proposes FastDriveVLA, a reconstruction-based visual token pruning framework for VLA models in autonomous driving. A plug-and-play module, ReconPruner, is trained with MAE-style pixel reconstruction and an adversarial foreground‚Äìbackground reconstruction strategy on the nuScenes-FG dataset, so that it can be attached to different VLA models to keep foreground-critical tokens and prune redundant ones, cutting computation while maintaining effective driving decisions.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-2507.23318-b31b1b?style=flat-square)](https://arxiv.org/abs/2507.23318) | ‚Äî | ‚Äî |
| **MCAM**<br><sub>Multimodal Causal Analysis Model for Ego-Vehicle-Level Driving Video Understanding ] ]</sub><br><sub><i>Summary:</i> MCAM provides a new solution for behavior understanding and causal reasoning in autonomous driving videos by integrating multimodal feature extraction, causal analysis, and vision-language converters.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2507.06072-b31b1b?style=flat-square)](https://arxiv.org/abs/2507.06072) | [![Stars](https://img.shields.io/github/stars/SixCorePeach/MCAM?style=social)](https://github.com/SixCorePeach/MCAM) | ‚Äî |
| **AutoDrive-R¬≤**<br><sub>Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving ]</sub><br><sub><i>Summary:</i> AutoDrive-R¬≤ enhances the reasoning and self-reflection capabilities of autonomous driving systems simultaneously by incorporating self-reflective thought chain processing and physics-based reinforcement learning.</sub> | ‚Äî | `RL` | [![arXiv](https://img.shields.io/badge/arXiv-2509.01944-b31b1b?style=flat-square)](https://arxiv.org/abs/2509.01944) | ‚Äî | ‚Äî |
| **DriveAgent-R1**<br><sub>Advancing VLM-based Autonomous Driving with Hybrid Thinking and Active Perception ]</sub><br><sub><i>Summary:</i> DriveAgent-R1 offers an innovative solution for the field of autonomous driving through a hybrid thinking and active perception mechanism, significantly enhancing the reliability and safety of decision-making in complex scenarios, while opening up new directions for future research.</sub> | ‚Äî | `VLM` ¬∑ `WorldModel` ¬∑ `Safety` | [![arXiv](https://img.shields.io/badge/arXiv-2507.20879-b31b1b?style=flat-square)](https://arxiv.org/abs/2507.20879) | ‚Äî | ‚Äî |
| **NavigScene**<br><sub>Bridging Local Perception and Global Navigation for Beyond-Visual-Range Autonomous Driving ]</sub><br><sub><i>Summary:</i> NavigScene significantly enhances the performance of autonomous driving systems by providing global navigation information, making them closer to the navigation capabilities of human drivers in complex and unknown environments.</sub> | ‚Äî | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2507.05227-b31b1b?style=flat-square)](https://arxiv.org/abs/2507.05227) | ‚Äî | ‚Äî |
| **ADRD**<br><sub>LLM-DRIVEN AUTONOMOUS DRIVING BASED ON RULE-BASED DECISION SYSTEMS ]</sub><br><sub><i>Summary:</i> ADRD (LLM-Driven Autonomous Driving with Rule-Based Decision Systems), a framework that leverages large language models to automatically generate rule-based driving policies, aims to achieve efficient, explainable, and robust autonomous driving decisions.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2506.14299-b31b1b?style=flat-square)](https://arxiv.org/abs/2506.14299) | ‚Äî | ‚Äî |
| **AutoVLA**<br><sub>A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning ] ] ]</sub><br><sub><i>Summary:</i> Reinforcement learning-based post-training methods and adaptive fast and slow thinking capabilities significantly improve planning performance</sub> | ‚Äî | `RL` ¬∑ `VLM` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2506.13757-b31b1b?style=flat-square)](https://arxiv.org/abs/2506.13757) | [![Stars](https://img.shields.io/github/stars/ucla-mobility/AutoVLA?style=social)](https://github.com/ucla-mobility/AutoVLA) | [Project](https://autovla.github.io/) |
| **Poutine**<br><sub>Vision-Language-Trajectory Pre-Training and Reinforcement Learning Post-Training Enable Robust End-to-End Autonomous Driving ]</sub><br><sub><i>Summary:</i> Poutine shows that both VLT pre-training and RL fine-tuning are critical to achieving strong driving performance in the long tail. This is a 3B parameter visual language model (VLM) designed for end-to-end autonomous driving in long-tail driving scenarios, trained with self-supervised visual language track (VLT) next tag prediction Poutine-Base</sub> | ‚Äî | `RL` ¬∑ `VLM` ¬∑ `WorldModel` | [![arXiv](https://img.shields.io/badge/arXiv-2506.11234-b31b1b?style=flat-square)](https://arxiv.org/abs/2506.11234) | ‚Äî | ‚Äî |
| **ReCogDrive**<br><sub>A Reinforced Cognitive Framework for End-to-End Autonomous Driving ] ] ]</sub><br><sub><i>Summary:</i> ReCogDrive, an autonomous driving system that combines the VLM with a diffusion planner</sub> | ‚Äî | `Diffusion` ¬∑ `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2506.08052-b31b1b?style=flat-square)](https://arxiv.org/abs/2506.08052) | [![Stars](https://img.shields.io/github/stars/xiaomi-research/recogdrive?style=social)](https://github.com/xiaomi-research/recogdrive) | [Project](https://xiaomi-research.github.io/recogdrive/) |
| **AD-EE**<br><sub>Early Exiting for Fast and Reliable Vision-Language Models in Autonomous Driving ]</sub><br><sub><i>Summary:</i> AD-EE is an early exit framework that incorporates domain characteristics of autonomous driving and uses causal reasoning to identify the optimal exit layer.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2506.05404-b31b1b?style=flat-square)](https://arxiv.org/abs/2506.05404) | ‚Äî | ‚Äî |
| **FastDrive**<br><sub>Structured Labeling Enables Faster Vision-Language Models for End-to-End Autonomous Driving ]</sub><br><sub><i>Summary:</i> Introducing NuScenes-S, a structured and concise benchmark dataset, FastDrive, a compact VLM baseline with 90 million parameters that can understand structured and concise descriptions and efficiently generate machine-friendly driving decisions</sub> | ‚Äî | `Dataset` ¬∑ `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2506.05442-b31b1b?style=flat-square)](https://arxiv.org/abs/2506.05442) | ‚Äî | ‚Äî |
| **HMVLM**<br><sub>Multistage Reasoning-Enhanced Vision-Language Model for Long-Tailed Driving Scenarios ]</sub><br><sub><i>Summary:</i> HaoMo implements the slow branch of cognitively inspired fast-slow architecture. The fast controller outputs low-level steering, throttle, and brake commands, while the slow planner (a large visual-language model) generates high-level intent.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2506.05883-b31b1b?style=flat-square)](https://arxiv.org/abs/2506.05883) | ‚Äî | ‚Äî |
| **S4-Driver**<br><sub>Scalable Self-Supervised Driving Multimodal Large Language Model with Spatio-Temporal Visual Representation, ]</sub><br><sub><i>Summary:</i> S4-Driver is a scalable self-supervised motion planning algorithm with spatiotemporal visual representations</sub> | 2025 / CVPR | `Planning` | [Paper](https://openaccess.thecvf.com//content/CVPR2025/papers/Xie_S4-Driver_Scalable_Self-Supervised_Driving_Multimodal_Large_Language_Model_with_Spatio-Temporal_CVPR_2025_paper.pdf) | ‚Äî | ‚Äî |
| **DiffVLA**<br><sub>Vision-Language Guided Diffusion Planning for Autonomous Driving ]</sub><br><sub><i>Summary:</i> Diff-VLA introduces a novel hybrid sparse-dense diffusion policy, enhanced by the integration of a Vision-Language Model (VLM)</sub> | ‚Äî | `Diffusion` ¬∑ `VLM` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2505.19381-b31b1b?style=flat-square)](https://arxiv.org/abs/2505.19381) | ‚Äî | ‚Äî |
| **X-Driver**<br><sub>Explainable Autonomous Driving with Vision-Language Models ]</sub><br><sub><i>Summary:</i> X-Driver is a unified multimodal large language model (MLLM) framework for closed-loop autonomous driving that utilizes Chain of Thought (CoT) reasoning and autoregressive modeling to improve both perception and decision-making performance.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2505.05098-b31b1b?style=flat-square)](https://arxiv.org/abs/2505.05098) | ‚Äî | ‚Äî |
| **DriveGPT4-V2**<br><sub>Harnessing Large Language Model Capabilities for Enhanced Closed-Loop Autonomous Driving, ]</sub><br><sub><i>Summary:</i> Different from the previous study DriveGPT4-V1, which focused on open-loop tasks, this study explores the ability of LLM in enhancing closed-loop autonomous driving and uses an expert LLM as a teacher for online policy supervision.</sub> | 2025 / CVPR | `VLM` ¬∑ `Planning` | [Paper](https://openaccess.thecvf.com//content/CVPR2025/papers/Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous_CVPR_2025_paper.pdf) | ‚Äî | ‚Äî |
| **DriveMind**<br><sub>A Dual-VLM based Reinforcement Learning Framework for Autonomous Driving ]</sub><br><sub><i>Summary:</i> A dynamic dual-VLM architecture is proposed, which combines a static contrastive VLM encoder with a novelty-triggered VLM encoder-decoder to solve the semantic rigidity problem of traditional fixed cues.</sub> | ‚Äî | `RL` ¬∑ `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2506.00819-b31b1b?style=flat-square)](https://arxiv.org/abs/2506.00819) | ‚Äî | ‚Äî |
| **ReasonPlan**<br><sub>Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving ] ]</sub><br><sub><i>Summary:</i> ReasonPlan is an MLLM fine-tuning framework specifically designed for closed-loop driving, enabling comprehensive reasoning through a self-supervised Next Scene Prediction task and a supervised Decision Chain-of-Thought process.</sub> | ‚Äî | `VLM` ¬∑ `WorldModel` | [![arXiv](https://img.shields.io/badge/arXiv-2505.20024-b31b1b?style=flat-square)](https://arxiv.org/abs/2505.20024) | [![Stars](https://img.shields.io/github/stars/Liuxueyi/ReasonPlan?style=social)](https://github.com/Liuxueyi/ReasonPlan) | ‚Äî |
| **FutureSightDrive**<br><sub>Thinking Visually with Spatio-Temporal CoT for Autonomous Driving ] ]</sub><br><sub><i>Summary:</i> FutureSightDrive proposes a spatio-temporal CoT reasoning method to enable the model to think visually.</sub> | ‚Äî | `WorldModel` | [![arXiv](https://img.shields.io/badge/arXiv-2505.17685-b31b1b?style=flat-square)](https://arxiv.org/abs/2505.17685) | [![Stars](https://img.shields.io/github/stars/missTL/FSDrive?style=social)](https://github.com/missTL/FSDrive) | ‚Äî |
| **PADriver**<br><sub>Towards Personalized Autonomous Driving ]</sub><br><sub><i>Summary:</i> Based on a multimodal large language model (MLLM), PADriver takes streaming video frames and personalized text prompts as input to actively perform scene understanding, danger level assessment, and action decision-making.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2505.05240-b31b1b?style=flat-square)](https://arxiv.org/abs/2505.05240) | ‚Äî | ‚Äî |
| **LDM**<br><sub>Unlock the Power of Unlabeled Data in Language Driving Model, ]</sub><br><sub><i>Summary:</i> Dynamic self-supervised pre-training framework, semi-supervised knowledge distillation architecture</sub> | 2025 / ICRA | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2503.10586-b31b1b?style=flat-square)](https://arxiv.org/abs/2503.10586) | ‚Äî | ‚Äî |
| **DriveMoE**<br><sub>Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving ] ] ]</sub><br><sub><i>Summary:</i> A new VLM-AD framework based on MoE</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2505.16278-b31b1b?style=flat-square)](https://arxiv.org/abs/2505.16278) | [![Stars](https://img.shields.io/github/stars/Thinklab-SJTU/DriveMoE?style=social)](https://github.com/Thinklab-SJTU/DriveMoE) | [Project](https://thinklab-sjtu.github.io/DriveMoE/) |
| **DriveMonkey**<br><sub>Extending Large Vision-Language Model for Diverse Interactive Tasks in Autonomous Driving ] ]</sub><br><sub><i>Summary:</i> Use a series of learnable queries to seamlessly integrate the LVLM with the spatial processor, which is designed as a plug-and-play component and can be initialized with a pre-trained 3D detector to improve 3D perception</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2505.08725-b31b1b?style=flat-square)](https://arxiv.org/abs/2505.08725) | [![Stars](https://img.shields.io/github/stars/zc-zhao/DriveMonkey?style=social)](https://github.com/zc-zhao/DriveMonkey) | ‚Äî |
| **AgentThink**<br><sub>A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving ]</sub><br><sub><i>Summary:</i> AgentThink combines Chain-of-Thought (CoT) reasoning with dynamic agent-style tool invocation for autonomous driving tasks for the first time.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2505.15298-b31b1b?style=flat-square)](https://arxiv.org/abs/2505.15298) | ‚Äî | ‚Äî |
| **DSDrive**<br><sub>Distilling Large Language Model for Lightweight End-to-End Autonomous Driving with Unified Reasoning and Planning ]</sub><br><sub><i>Summary:</i> DSDrive uses a distillation method to enhance lightweight LLM as the core of the AD system</sub> | ‚Äî | `VLM` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2505.05360-b31b1b?style=flat-square)](https://arxiv.org/abs/2505.05360) | ‚Äî | ‚Äî |
| **LightEMMA**<br><sub>Lightweight End-to-end Multimodal Autonomous Driving ] ]</sub><br><sub><i>Summary:</i> LightEMMA is a lightweight, end-to-end multimodal model designed for autonomous driving, enabling efficient and comprehensive perception and decision-making.</sub> | ‚Äî | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2505.00284-b31b1b?style=flat-square)](https://arxiv.org/abs/2505.00284) | [![Stars](https://img.shields.io/github/stars/michigan-traffic-lab/LightEMMA?style=social)](https://github.com/michigan-traffic-lab/LightEMMA) | ‚Äî |
| **Towards Human-Centric Autonomous Driving**<br><sub>A Fast-Slow Architecture Integrating Large Language Model Guidance with Reinforcement Learning ]</sub><br><sub><i>Summary:</i> A "fast and slow" decision-making framework that combines a large language model (LLM) for high-level instruction parsing and a reinforcement learning (RL) agent for low-level real-time decision-making.</sub> | ‚Äî | `RL` ¬∑ `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2505.06875-b31b1b?style=flat-square)](https://arxiv.org/abs/2505.06875) | ‚Äî | ‚Äî |
| **DriveSOTIF**<br><sub>Advancing Perception SOTIF Through Multimodal Large Language Models ]</sub><br><sub><i>Summary:</i> The first innovative fusion of multimodal large language models (MLLMs) and SOTIF risk recognition</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2505.07084-b31b1b?style=flat-square)](https://arxiv.org/abs/2505.07084) | ‚Äî | ‚Äî |
| **Actor-Reasoner**<br><sub>Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions ] ]</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2503.00502-b31b1b?style=flat-square)](https://arxiv.org/abs/2503.00502) | [![Stars](https://img.shields.io/github/stars/FanGShiYuu/Actor-Reasoner?style=social)](https://github.com/FanGShiYuu/Actor-Reasoner) | ‚Äî |
| **MPDrive**<br><sub>Improving Spatial Understanding with Marker-Based Prompt Learning for Autonomous Driving, ]</sub><br><sub><i>Summary:</i> By using detection experts to overlay numerical labels on target regions to create labeled images, we transform complex text coordinate generation into text-based visual label prediction.</sub> | 2025 / CVPR | `WorldModel` | [![arXiv](https://img.shields.io/badge/arXiv-2504.00379-b31b1b?style=flat-square)](https://arxiv.org/abs/2504.00379) | ‚Äî | ‚Äî |
| **V3LMA**<br><sub>Visual 3D-enhanced Language Model for Autonomous Driving ]</sub><br><sub><i>Summary:</i> Approach improves 3D scene understanding by combining Large Language Models (LLMs) with vision-language models (LVLMs).</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2505.00156-b31b1b?style=flat-square)](https://arxiv.org/abs/2505.00156) | ‚Äî | ‚Äî |
| **OpenDriveVLA**<br><sub>Towards End-to-end Autonomous Driving with Large Vision Language Action Model ] ] ]</sub><br><sub><i>Summary:</i> OpenDriveVLA is built on an open-source pre-trained large-scale vision-language model (VLM) to generate reliable driving actions conditioned on 3D environment perception, ego vehicle state, and driver commands.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2503.23463-b31b1b?style=flat-square)](https://arxiv.org/abs/2503.23463) | [![Stars](https://img.shields.io/github/stars/DriveVLA/OpenDriveVLA?style=social)](https://github.com/DriveVLA/OpenDriveVLA) | [Project](https://drivevla.github.io/) |
| **SimLingo**<br><sub>Vision-Only Closed-Loop Autonomous Driving with Language-Action Alignment, ] ] ]</sub><br><sub><i>Summary:</i> SimLingo is a vision-language-action model unifying the tasks of autonomous driving, vision-language understanding and language-action alignment.</sub> | 2025 / CVPR | `VLM` | [Paper](https://openaccess.thecvf.com//content/CVPR2025/papers/Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment_CVPR_2025_paper.pdf) | [![Stars](https://img.shields.io/github/stars/RenzKa/simlingo?style=social)](https://github.com/RenzKa/simlingo) | [Project](https://www.katrinrenz.de/simlingo/) |
| **SAFEAUTO**<br><sub>KNOWLEDGE-ENHANCED SAFE AUTONOMOUS DRIVING WITH MULTIMODAL FOUNDATION MODELS , ] ]</sub><br><sub><i>Summary:</i> SAFEAUTO introduces the Place-Dependent Cross-Entropy (PDCE) loss function, specifically designed to improve the accuracy of low-level control signal predictions by treating numerical values as textual sequences.</sub> | 2025 / ICLR | `WorldModel` ¬∑ `Safety` | [![arXiv](https://img.shields.io/badge/arXiv-2503.00211-b31b1b?style=flat-square)](https://arxiv.org/abs/2503.00211) | [![Stars](https://img.shields.io/github/stars/AI-secure/SafeAuto?style=social)](https://github.com/AI-secure/SafeAuto) | ‚Äî |
| **NuGrounding**<br><sub>A Multi-View 3D Visual Grounding Framework in Autonomous Driving ]</sub><br><sub><i>Summary:</i> NuGrounding introduces a novel paradigm that seamlessly integrates the instruction comprehension capabilities of multimodal large language models (MLLMs) with the precise localization abilities of specialized detection models.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2503.22436-b31b1b?style=flat-square)](https://arxiv.org/abs/2503.22436) | ‚Äî | ‚Äî |
| **CoT-Drive**<br><sub>Efficient Motion Forecasting for Autonomous Driving with LLMs and Chain-of-Thought Prompting ]</sub><br><sub><i>Summary:</i> Use LLMs and chaining cues to do prediction tasks</sub> | ‚Äî | `VLM` ¬∑ `WorldModel` | [![arXiv](https://img.shields.io/badge/arXiv-2503.07234-b31b1b?style=flat-square)](https://arxiv.org/abs/2503.07234) | ‚Äî | ‚Äî |
| **CoLMDriver**<br><sub>LLM-based Negotiation Benefits Cooperative Autonomous Driving ] ]</sub><br><sub><i>Summary:</i> The first full-process collaborative driving system based on a large language model, capable of effective language-based negotiation and real-time driving control.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2503.08683-b31b1b?style=flat-square)](https://arxiv.org/abs/2503.08683) | [![Stars](https://img.shields.io/github/stars/cxliu0314/CoLMDriver?style=social)](https://github.com/cxliu0314/CoLMDriver) | ‚Äî |
| **AlphaDrive**<br><sub>Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning ] ]</sub><br><sub><i>Summary:</i> AlphaDrive is the first framework to integrate GRPO-based RL and planning reasoning into autonomous driving</sub> | ‚Äî | `RL` ¬∑ `VLM` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2503.07608-b31b1b?style=flat-square)](https://arxiv.org/abs/2503.07608) | [![Stars](https://img.shields.io/github/stars/hustvl/AlphaDrive?style=social)](https://github.com/hustvl/AlphaDrive) | ‚Äî |
| **TrackingMeetsLMM**<br><sub>Tracking Meets Large Multimodal Models for Driving Scenario Understanding ] ]</sub><br><sub><i>Summary:</i> Introduced a novel method to embed tracking information into LMMs to enhance their spatiotemporal understanding of driving scenarios</sub> | ‚Äî | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2503.14498-b31b1b?style=flat-square)](https://arxiv.org/abs/2503.14498) | [![Stars](https://img.shields.io/github/stars/mbzuai-oryx/TrackingMeetsLMM?style=social)](https://github.com/mbzuai-oryx/TrackingMeetsLMM) | ‚Äî |
| **BEVDriver**<br><sub>Leveraging BEV Maps in LLMs for Robust Closed-Loop Driving ]</sub><br><sub><i>Summary:</i> Directly utilize the original BEV features generated by LiDAR and camera to eliminate the dependence on pre-predicted path points. Use two PIDs to control the lateral and longitudinal directions to bridge the gap between high-level decision-making and low-level planning.</sub> | ‚Äî | `VLM` ¬∑ `Planning` ¬∑ `BEV` | [![arXiv](https://img.shields.io/badge/arXiv-2503.03074-b31b1b?style=flat-square)](https://arxiv.org/abs/2503.03074) | ‚Äî | ‚Äî |
| **DynRsl-VLM**<br><sub>Enhancing Autonomous Driving Perception with Dynamic Resolution Vision-Language Models ]</sub><br><sub><i>Summary:</i> DynRsl-VLM incorporates a dynamic resolution image input processing approach that captures all entity feature information within an image while ensuring that the image input remains computationally tractable for the Vision Transformer (ViT).</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2503.11265-b31b1b?style=flat-square)](https://arxiv.org/abs/2503.11265) | ‚Äî | ‚Äî |
| **Sce2DriveX**<br><sub>A Generalized MLLM Framework for Scene-to-Drive Learning ]</sub><br><sub><i>Summary:</i> Sce2DriveX utilizes multimodal joint learning from local scene videos and global BEV maps to deeply understand long-range spatiotemporal relationships and road topology, enhancing its comprehensive perception and reasoning capabilities in 3D dynamic/static scenes and achieving driving generalization across scenes. - VLM-Assisted Continual learning for Visual Question Answering in Self-Driving  [[Paper](https://arxiv.org/pdf/2502.00843)] Summary: It introduces a novel continual learning framework that integrates vision-language models (VLMs) with selective memory replay and knowledge distillation, further strengthened by regularization of task-specific projection layers.</sub> | ‚Äî | `VLM` ¬∑ `BEV` | [![arXiv](https://img.shields.io/badge/arXiv-2502.14917-b31b1b?style=flat-square)](https://arxiv.org/abs/2502.14917) | ‚Äî | ‚Äî |
| **LeapVAD**<br><sub>A Leap in Autonomous Driving via Cognitive Perception and Dual-Process Thinking ] ] ]</sub><br><sub><i>Summary:</i> LeapAD is a dual-process, closed-loop autonomous driving system that enables continuous learning, adaptation, and improvement over time.</sub> | ‚Äî | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2501.08168-b31b1b?style=flat-square)](https://arxiv.org/abs/2501.08168) | [![Stars](https://img.shields.io/github/stars/PJLab-ADG/LeapVAD?style=social)](https://github.com/PJLab-ADG/LeapVAD) | [Project](https://pjlab-adg.github.io/LeapVAD/) |
</details>

<details open>
<summary>2024</summary>

| üß† **Method** | üóìÔ∏è **Year / Venue** | üè∑Ô∏è **Tags** | üìÑ **Paper** | üíª **GitHub** | üåê **Project** |
|---|---|---|---|---|---|
| **VLM-RL**<br><sub>A Unified Vision Language Model and Reinforcement Learning Framework for Safe Autonomous Driving]]]</sub> | ‚Äî | `RL` ¬∑ `VLM` ¬∑ `Safety` | [![arXiv](https://img.shields.io/badge/arXiv-2412.15544-b31b1b?style=flat-square)](https://arxiv.org/abs/2412.15544) | [![Stars](https://img.shields.io/github/stars/zihaosheng/VLM-RL?style=social)](https://github.com/zihaosheng/VLM-RL) | [Project](https://www.huang-zilin.com/VLM-RL-website/) |
| **GPVL**<br><sub>Generative Planning with 3D-vision Language Pre-training for End-to-End Autonomous Driving, ] ]</sub><br><sub><i>Summary:</i> A generative planning framework for autonomous driving using a 3D vision-language pre-training paradigm.</sub> | 2025 / AAAI | `VLM` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2501.08861-b31b1b?style=flat-square)](https://arxiv.org/abs/2501.08861) | [![Stars](https://img.shields.io/github/stars/ltp1995/GPVL?style=social)](https://github.com/ltp1995/GPVL) | ‚Äî |
| **CALMM-Drive**<br><sub>Confidence-Aware Autonomous Driving with Large Multimodal Model ]</sub><br><sub><i>Summary:</i> The CALMM-Drive approach integrates driving task-specific Chain-of-Thought (CoT) reasoning with Top-K confidence elicitation to improve the accuracy and reliability of decision-making.</sub> | ‚Äî | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2412.04209-b31b1b?style=flat-square)](https://arxiv.org/abs/2412.04209) | ‚Äî | ‚Äî |
| **WiseAD**<br><sub>Knowledge Augmented End-to-End Autonomous Driving with Vision-Language Model ] ]</sub><br><sub><i>Summary:</i> WiseAD is a specialized vision-language model (VLM) designed for end-to-end autonomous driving, capable of performing driving reasoning, action justification, object recognition, risk analysis, providing driving suggestions, and trajectory planning across a wide range of scenarios.</sub> | ‚Äî | `VLM` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2412.09951-b31b1b?style=flat-square)](https://arxiv.org/abs/2412.09951) | [![Stars](https://img.shields.io/github/stars/wyddmw/WiseAD?style=social)](https://github.com/wyddmw/WiseAD) | ‚Äî |
| **OpenEMMA**<br><sub>Open-Source Multimodal Model for End-to-End Autonomous Driving, ] ]</sub><br><sub><i>Summary:</i> OpenEMMA leverages existing open source modules and pre-trained MLLMs to replicate the capabilities of EMMA in trajectory planning and perception.</sub> | 2025 / WACV | `VLM` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2412.15208-b31b1b?style=flat-square)](https://arxiv.org/abs/2412.15208) | [![Stars](https://img.shields.io/github/stars/taco-group/OpenEMMA?style=social)](https://github.com/taco-group/OpenEMMA) | ‚Äî |
| **FeD**<br><sub>Feedback-Guided Autonomous Driving, ]]</sub><br><sub><i>Summary:</i> Achieving the First Perceptual-Motion End-to-End Training and Evaluation of an LLM-Based Driving Model</sub> | 2024 / CVPR | `VLM` | [Paper](https://fedaltothemetal.github.io/resources/FeD_v1.pdf) | ‚Äî | [Project](https://fedaltothemetal.github.io/) |
| **LeapAD**<br><sub>Continuously learning, adapting, and improving: A dual-process approach to autonomous driving, ]]]</sub> | 2024 / NeurIPS | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2405.15324-b31b1b?style=flat-square)](https://arxiv.org/abs/2405.15324) | [![Stars](https://img.shields.io/github/stars/PJLab-ADG/LeapAD?style=social)](https://github.com/PJLab-ADG/LeapAD) | [Project](https://pjlab-adg.github.io/LeapAD/) |
| **DriveMM**<br><sub>All-in-One Large Multimodal Model for Autonomous Driving]]]</sub><br><sub><i>Summary:</i> DriveMM is robustly designed with the general capability to perform a wide variety of autonomous driving (AD) tasks and demonstrates strong generalization performance, enabling effective transfer to new datasets. - Explanation for Trajectory Planning using Multi-modal Large Language Model for Autonomous Driving**, ECCV2024**[[Paper](https://arxiv.org/abs/2411.09971)] Summary:  Leveraging the newly collected dataset, we take the future planning trajectory of the ego vehicle as input.</sub> | ‚Äî | `Dataset` ¬∑ `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2412.07689-b31b1b?style=flat-square)](https://arxiv.org/abs/2412.07689) | [![Stars](https://img.shields.io/github/stars/zhijian11/DriveMM?style=social)](https://github.com/zhijian11/DriveMM) | [Project](https://zhijian11.github.io/DriveMM/) |
| **LaVida Drive**<br><sub>Vision-Text Interaction VLM for Autonomous Driving with Token Selection, Recovery and Enhancement ]</sub><br><sub><i>Summary:</i> An innovative VQA framework designed to support fine-grained perception of high-resolution visual inputs in dynamic driving environments while integrating temporal information.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2411.12980-b31b1b?style=flat-square)](https://arxiv.org/abs/2411.12980) | ‚Äî | ‚Äî |
| **EMMA**<br><sub>End-to-End Multimodal Model for Autonomous Driving ]</sub><br><sub><i>Summary:</i> EMMA directly maps raw camera sensor data into various driving-specific outputs, including planner trajectories, perception objects, and road graph elements.</sub> | ‚Äî | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2410.23262-b31b1b?style=flat-square)](https://arxiv.org/abs/2410.23262) | ‚Äî | ‚Äî |
| **DriVLMe**<br><sub>Enhancing LLM-based Autonomous Driving Agents with Embodied and Social Experiences, ] ] ]</sub><br><sub><i>Summary:</i> DriVLMe is a video-language model-based agent designed to enable natural and effective communication between humans and autonomous vehicles, allowing the vehicles to perceive their surroundings and navigate the environment more intuitively.</sub> | 2024 / IROS | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2406.03008-b31b1b?style=flat-square)](https://arxiv.org/abs/2406.03008) | [![Stars](https://img.shields.io/github/stars/sled-group/driVLMe?style=social)](https://github.com/sled-group/driVLMe/tree/main) | [Project](https://sled-group.github.io/driVLMe/) |
| **OccLLaMA**<br><sub>An Occupancy-Language-Action Generative World Model for Autonomous Driving ]</sub><br><sub><i>Summary:</i> OccLLaMA is a unified 3D occupancy-language-action generative world model that integrates various VLA (vision-language-action) related tasks.</sub> | ‚Äî | `VLM` ¬∑ `WorldModel` | [![arXiv](https://img.shields.io/badge/arXiv-2409.03272-b31b1b?style=flat-square)](https://arxiv.org/abs/2409.03272) | ‚Äî | ‚Äî |
| **MiniDrive**<br><sub>More Efficient Vision-Language Models with Multi-Level 2D Features as Text Tokens for Autonomous Driving]</sub><br><sub><i>Summary:</i> By combining the Feature Engineering Mixture of Experts (FEMoE) module with a dynamic instruction adapter, our approach addresses the limitation of previous methods, which could only generate static visual token embeddings for a given image.</sub> | ‚Äî | `VLM` ¬∑ `Imitation` | [![arXiv](https://img.shields.io/badge/arXiv-2409.07267-b31b1b?style=flat-square)](https://arxiv.org/abs/2409.07267) | ‚Äî | ‚Äî |
| **RDA-Driver**<br><sub>Making Large Language Models Better Planners with Reasoning-Decision Alignment, ]</sub><br><sub><i>Summary:</i> We develop an end-to-end decision model based on a multimodal enhanced LLM that simultaneously performs CoT reasoning and enforces planning outcomes.</sub> | 2024 / ECCV | `VLM` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2408.13890-b31b1b?style=flat-square)](https://arxiv.org/abs/2408.13890) | ‚Äî | ‚Äî |
| **EC-Drive**<br><sub>Edge-Cloud Collaborative Motion Planning for Autonomous Driving with Large Language Models, ]]</sub><br><sub><i>Summary:</i> EC-Drive utilizes drift detection algorithms to selectively upload critical data, including new obstacles and traffic pattern changes, to the cloud for processing by GPT-4, while routine data is efficiently managed by smaller LLMs on edge devices.</sub> | 2024 / ICCT | `VLM` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2408.09972-b31b1b?style=flat-square)](https://arxiv.org/abs/2408.09972) | ‚Äî | [Project](https://sites.google.com/view/ec-drive) |
| **V2X-VLM**<br><sub>End-to-End V2X Cooperative Autonomous Driving Through Large Vision-Language Models]]]</sub><br><sub><i>Summary:</i> This study aims to propose pioneering E2E vehicle-infrastructure cooperative autonomous driving (VICAD) framework leveraging large VLMs to enhance collaborative situational awareness, decision-making, and overall driving performances.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2408.09251-b31b1b?style=flat-square)](https://arxiv.org/abs/2408.09251) | [![Stars](https://img.shields.io/github/stars/zilin-huang/V2X-VLM?style=social)](https://github.com/zilin-huang/V2X-VLM) | [Project](https://www.huang-zilin.com/V2X-VLM-website/) |
| **Cube-LLM**<br><sub>Language-Image Models with 3D Understanding]]</sub><br><sub><i>Summary:</i> Cube-LLM, a pre-trained visual language model for autonomous driving, can infer 3D indoor and outdoor scenes from a single image</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2405.03685-b31b1b?style=flat-square)](https://arxiv.org/abs/2405.03685) | ‚Äî | [Project](https://janghyuncho.github.io/Cube-LLM/) |
| **VLM-MPC**<br><sub>Vision Language Foundation Model (VLM)-Guided Model Predictive Controller (MPC) for Autonomous Driving]</sub><br><sub><i>Summary:</i> VLM-MPC combines the Model Predictive Controller (MPC) with VLM to evaluate how model-based control could enhance VLM decision-making.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2408.04821-b31b1b?style=flat-square)](https://arxiv.org/abs/2408.04821) | ‚Äî | ‚Äî |
| **SimpleLLM4AD**<br><sub>An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving, ]</sub><br><sub><i>Summary:</i> SimpleLLM4AD reimagines the traditional autonomous driving pipeline by structuring the task into four interconnected stages: perception, prediction, planning, and behavior.</sub> | IEIT Systems | `VLM` ¬∑ `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2407.21293-b31b1b?style=flat-square)](https://arxiv.org/abs/2407.21293) | ‚Äî | ‚Äî |
| **AsyncDriver**<br><sub>Asynchronous Large Language Model Enhanced Planner for Autonomous Driving, ]]</sub><br><sub><i>Summary:</i> AsyncDriver is a novel asynchronous, LLM-enhanced closed-loop framework that utilizes scene-aware instruction features generated by a large language model (LLM) to guide real-time planners in producing accurate and controllable trajectory predictions.</sub> | 2024 / ECCV | `VLM` ¬∑ `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2406.14556-b31b1b?style=flat-square)](https://arxiv.org/abs/2406.14556) | [![Stars](https://img.shields.io/github/stars/memberRE/AsyncDriver?style=social)](https://github.com/memberRE/AsyncDriver) | ‚Äî |
| **AD-H**<br><sub>AUTONOMOUS DRIVING WITH HIERARCHICAL AGENTS , ]</sub><br><sub><i>Summary:</i> A hierarchical framework that facilitates collaboration between two agents: the MLLM-based planner and the controller.</sub> | 2025 / ICLR | `VLM` | [Paper](https://openreview.net/pdf/e15ef4c8e8f4e0d2db875b42314bcc25546c73dc.pdf) | ‚Äî | ‚Äî |
| **CarLLaVA**<br><sub>Vision language models for camera-only closed-loop driving]]</sub><br><sub><i>Summary:</i> CarLLaVA uses a semi-disentangled output representation of both path predictions and waypoints, getting the advantages of the path for better lateral control and the waypoints for better longitudinal control.</sub> | ‚Äî | `WorldModel` | [![arXiv](https://img.shields.io/badge/arXiv-2406.10165-b31b1b?style=flat-square)](https://arxiv.org/abs/2406.10165) | ‚Äî | [Project](https://www.youtube.com/watch?v=E1nsEgcHRuc) |
| **PlanAgent**<br><sub>A Multi-modal Large Language Agent for Closed-loop Vehicle Motion Planning]</sub><br><sub><i>Summary:</i> PlanAgent is the first closed-loop mid-to-mid(use bev, no raw sensor) autonomous driving planning agent system based on a Multi-modal Large Language Model.</sub> | ‚Äî | `Planning` ¬∑ `BEV` | [![arXiv](https://img.shields.io/badge/arXiv-2406.01587-b31b1b?style=flat-square)](https://arxiv.org/abs/2406.01587) | ‚Äî | ‚Äî |
| **Atlas**<br><sub>Is a 3D-Tokenized LLM the Key to Reliable Autonomous Driving?]</sub><br><sub><i>Summary:</i> A DETR-style 3D perceptron is introduced as a 3D tokenizer, which connects LLM with a single-layer linear projector.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2405.18361-b31b1b?style=flat-square)](https://arxiv.org/abs/2405.18361) | ‚Äî | ‚Äî |
| **Driving with Regulation**<br><sub>Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM]</sub><br><sub><i>Summary:</i> Traffic Regulation Retrieval (TRR) agent based on Retrieval Augmented Generation (RAG) to automatically retrieve relevant traffic rules and guidelines from a wide range of regulatory documents and related records based on the context of the autonomous vehicle</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2410.04759-b31b1b?style=flat-square)](https://arxiv.org/abs/2410.04759) | ‚Äî | ‚Äî |
| **OmniDrive**<br><sub>A Holistic Vision-Language Dataset for Autonomous Driving with Counterfactual Reasoning, ]]</sub><br><sub><i>Summary:</i> The  features a novel 3D multimodal LLM design that uses sparse queries to lift and compress visual representations into 3D.</sub> | 2025 / CVPR | `Dataset` ¬∑ `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2405.01533-b31b1b?style=flat-square)](https://arxiv.org/abs/2405.01533) | [![Stars](https://img.shields.io/github/stars/NVlabs/OmniDrive?style=social)](https://github.com/NVlabs/OmniDrive) | ‚Äî |
| **Co-driver**<br><sub>VLM-based Autonomous Driving Assistant with Human-like Behavior and Understanding for Complex Road Scenes]</sub><br><sub><i>Summary:</i> This is an automated driving assistance system that provides adjustable driving behavior for autonomous vehicles based on an understanding of complex road scenarios, including safety distances, weather, lighting conditions, road surfaces, and locations.</sub> | ‚Äî | `VLM` ¬∑ `Safety` | [Paper](https://arxiv.org/html/2405.05885v1) | ‚Äî | ‚Äî |
| **AgentsCoDriver**<br><sub>Large Language Model Empowered Collaborative Driving with Lifelong Learning]</sub><br><sub><i>Summary:</i> Multiple vehicles are capable of collaborative driving It can accumulate knowledge, lessons, and experiences over time by constantly interacting with its environment, enabling lifelong learning</sub> | ‚Äî | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2404.06345-b31b1b?style=flat-square)](https://arxiv.org/abs/2404.06345) | ‚Äî | ‚Äî |
| **EM-VLM4AD**<br><sub>Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving, ]]</sub><br><sub><i>Summary:</i> EM-VLM4AD is an efficient and lightweight multi-frame vision-language model designed to perform visual question answering for autonomous driving applications.</sub> | 2024 / CVPR | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2403.19838-b31b1b?style=flat-square)](https://arxiv.org/abs/2403.19838) | [![Stars](https://img.shields.io/github/stars/akshaygopalkr/EM-VLM4AD?style=social)](https://github.com/akshaygopalkr/EM-VLM4AD) | ‚Äî |
| **LeGo-Drive**<br><sub>Language-enhanced Goal-oriented Closed-Loop End-to-End Autonomous Driving, ]]]</sub><br><sub><i>Summary:</i> A novel planning-guided end-to-end LLM-based goal point navigation solution that predicts and improves the desired state by dynamically interacting with the environment and generating a collision-free trajectory.</sub> | 2024 / IROS | `VLM` ¬∑ `Planning` ¬∑ `Safety` | [![arXiv](https://img.shields.io/badge/arXiv-2403.20116-b31b1b?style=flat-square)](https://arxiv.org/abs/2403.20116) | [![Stars](https://img.shields.io/github/stars/reachpranjal/lego-drive?style=social)](https://github.com/reachpranjal/lego-drive) | [Project](https://reachpranjal.com/lego-drive/) |
| **Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving**<br><sub>]</sub><br><sub><i>Summary:</i> Regarding the "location of the object," "speed of our car," "distance to the object," and "our car‚Äôs direction" are fed into the large language model for mathematical calculations within CARLA. After formulating these calculations based on overcoming weather conditions, precise control values for brake and speed are generated.</sub> | 2024 / ICCMA | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2402.13602-b31b1b?style=flat-square)](https://arxiv.org/abs/2402.13602) | ‚Äî | ‚Äî |
| **VLAAD**<br><sub>Vision and Language Assistant for Autonomous Driving, ]</sub><br><sub><i>Summary:</i> Aiming to enhance the explainability of autonomous driving systems.</sub> | 2024 / WACV | ‚Äî | [Paper](https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/papers/Park_VLAAD_Vision_and_Language_Assistant_for_Autonomous_Driving_WACVW_2024_paper.pdf) | ‚Äî | ‚Äî |
| **ELM**<br><sub>Embodied Understanding of Driving Scenarios, ]</sub><br><sub><i>Summary:</i> we introduce the Embodied Language Model (ELM), a comprehensive framework tailored for agents' understanding of driving scenes with large spatial and temporal spans.</sub> | 2024 / ECCV | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2403.04593-b31b1b?style=flat-square)](https://arxiv.org/abs/2403.04593) | ‚Äî | ‚Äî |
| **RAG-Driver**<br><sub>Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model,]]]</sub><br><sub><i>Summary:</i> RAG-Driver is a novel retrieval-augmented, multimodal large language model that utilizes in-context learning to enable high-performance, interpretable, and generalizable autonomous driving.</sub> | 2024 / RSS | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2402.10828-b31b1b?style=flat-square)](https://arxiv.org/abs/2402.10828) | [![Stars](https://img.shields.io/github/stars/YuanJianhao508/RAG-Driver?style=social)](https://github.com/YuanJianhao508/RAG-Driver) | [Project](https://yuanjianhao508.github.io/RAG-Driver/) |
| **BEV-TSR**<br><sub>Text-Scene Retrieval in BEV Space for Autonomous Driving,]</sub><br><sub><i>Summary:</i> Focus on enhancing the semantic capabilities of BEV representations</sub> | 2025 / AAAI- | `BEV` | [![arXiv](https://img.shields.io/badge/arXiv-2401.01065-b31b1b?style=flat-square)](https://arxiv.org/abs/2401.01065) | ‚Äî | ‚Äî |
| **LLaDA**<br><sub>Driving Everywhere with Large Language Model Policy Adaptation, ]]]</sub><br><sub><i>Summary:</i> Traffic Rule Extractor (TRE), which aims to organize and filter the inputs (initial plan+unique traffic code) and feed the output into the frozen LLMs to obtain the final new plan.</sub> | 2024 / CVPR | `VLM` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2402.05932-b31b1b?style=flat-square)](https://arxiv.org/abs/2402.05932) | [![Stars](https://img.shields.io/github/stars/Boyiliee/LLaDA-AV?style=social)](https://github.com/Boyiliee/LLaDA-AV) | [Project](https://boyiliee.github.io/llada/) |
</details>

<details open>
<summary>2023</summary>

| üß† **Method** | üóìÔ∏è **Year / Venue** | üè∑Ô∏è **Tags** | üìÑ **Paper** | üíª **GitHub** | üåê **Project** |
|---|---|---|---|---|---|
| **LingoQA**<br><sub>Visual Question Answering for Autonomous Driving, ] ]</sub><br><sub><i>Summary:</i> This is a novel framework for integrating LLM into AD systems, enabling them to follow user instructions by generating code that leverages established functional primitives.</sub> | 2024 / ECCV | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2312.14115-b31b1b?style=flat-square)](https://arxiv.org/abs/2312.14115) | [![Stars](https://img.shields.io/github/stars/wayveai/LingoQA?style=social)](https://github.com/wayveai/LingoQA/) | ‚Äî |
| **LaMPilot**<br><sub>An Open Benchmark Dataset for Autonomous Driving with Language Model Programs, ] ]</sub><br><sub><i>Summary:</i> This is a novel framework for integrating LLM into AD systems, enabling them to follow user instructions by generating code that leverages established functional primitives.</sub> | 2024 / CVPR | `Dataset` ¬∑ `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2312.04372-b31b1b?style=flat-square)](https://arxiv.org/abs/2312.04372) | [![Stars](https://img.shields.io/github/stars/PurdueDigitalTwin/LaMPilot?style=social)](https://github.com/PurdueDigitalTwin/LaMPilot)) | [Project](https://github.com/PurdueDigitalTwin/LaMPilot) |
| **LLM-ASSIST**<br><sub>Enhancing Closed-Loop Planning with Language-Based Reasoning ] ]</sub><br><sub><i>Summary:</i> LLM-Planner takes over scenarios that PDM-Closed cannot handle.</sub> | ‚Äî | `VLM` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2401.00125-b31b1b?style=flat-square)](https://arxiv.org/abs/2401.00125) | ‚Äî | [Project](https://llmassist.github.io/) |
| **DriveLM**<br><sub>Driving with Graph Visual Question Answering, ] ]</sub><br><sub><i>Summary:</i> Graph VQA involves formulating Perception, Prediction, Planning reasoning as a series of questionanswer pairs (QAs) in a directed graph.</sub> | 2024 / ECCV  Oral | `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2312.14150-b31b1b?style=flat-square)](https://arxiv.org/abs/2312.14150) | [![Stars](https://img.shields.io/github/stars/OpenDriveLab/DriveLM?style=social)](https://github.com/OpenDriveLab/DriveLM) | ‚Äî |
| **DriveMLM**<br><sub>Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving ] ]</sub><br><sub><i>Summary:</i> Design an MLLM planner for decision prediction, and develop a data engine that can effectively generate decision states and corresponding explanation annotation for model training and evaluation.</sub> | ‚Äî | `VLM` ¬∑ `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2312.09245-b31b1b?style=flat-square)](https://arxiv.org/abs/2312.09245) | [![Stars](https://img.shields.io/github/stars/OpenGVLab/DriveMLM?style=social)](https://github.com/OpenGVLab/DriveMLM) | ‚Äî |
| **LiDAR-LLM**<br><sub>Exploring the Potential of Large Language Models for 3D LiDAR Understanding ] ]</sub><br><sub><i>Summary:</i> Take raw LiDAR data as input and leverage LLM‚Äôs superior inference capabilities to fully understand outdoor 3D scenes.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2312.14074-b31b1b?style=flat-square)](https://arxiv.org/abs/2312.14074) | ‚Äî | [Project](https://sites.google.com/view/lidar-llm) |
| **Talk2BEV**<br><sub>Language-enhanced Bird's-eye View Maps for Autonomous Driving ] ] ]</sub><br><sub><i>Summary:</i> Large-scale visual language model (LVLM) combined with BEV map to achieve visual reasoning, spatial understanding and decision making.</sub> | ‚Äî | `VLM` ¬∑ `BEV` | [![arXiv](https://img.shields.io/badge/arXiv-2310.02251-b31b1b?style=flat-square)](https://arxiv.org/abs/2310.02251) | [![Stars](https://img.shields.io/github/stars/llmbev/talk2bev?style=social)](https://github.com/llmbev/talk2bev) | [Project](https://llmbev.github.io/talk2bev/) |
| **Talk2Drive**<br><sub>Personalized Autonomous Driving with Large Language Models: Field Experiments ] ]</sub><br><sub><i>Summary:</i> capable of translating natural verbal commands into executable controls and learning to satisfy personal preferences for safety, efficiency, and comfort with a proposed memory module.</sub> | ‚Äî | `Safety` | [![arXiv](https://img.shields.io/badge/arXiv-2312.09397-b31b1b?style=flat-square)](https://arxiv.org/abs/2312.09397) | ‚Äî | [Project](https://www.youtube.com/watch?v=4BWsfPaq1Ro) |
| **LMDrive**<br><sub>Closed-Loop End-to-End Driving with Large Language Models, ] ]</sub><br><sub><i>Summary:</i> LMDrive, the very first work to leverage LLMs for closed-loop end-to-end autonomous driving.</sub> | 2024 / CVPR | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2312.07488-b31b1b?style=flat-square)](https://arxiv.org/abs/2312.07488) | [![Stars](https://img.shields.io/github/stars/opendilab/LMDrive?style=social)](https://github.com/opendilab/LMDrive) | ‚Äî |
| **Reason2Drive**<br><sub>Towards Interpretable and Chain-based Reasoning for Autonomous Driving, ] ]</sub><br><sub><i>Summary:</i> Introduce a straightforward yet effective framework that enhances existing VLMs with two new components: a prior tokenizer and an instructed vision decoder.</sub> | 2024 / ECCV | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2312.03661-b31b1b?style=flat-square)](https://arxiv.org/abs/2312.03661) | [![Stars](https://img.shields.io/github/stars/fudan-zvg/Reason2Drive?style=social)](https://github.com/fudan-zvg/Reason2Drive) | ‚Äî |
| **CAVG**<br><sub>GPT-4 Enhanced Multimodal Grounding for Autonomous Driving: Leveraging Cross-Modal Attention with Large Language Models ] ]</sub><br><sub><i>Summary:</i> Utilize five encoders: Text, Image, Context, and Cross-Modal‚Äîwith: with a Multimodal decoder to predict object bounding box.</sub> | ‚Äî | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2312.03543-b31b1b?style=flat-square)](https://arxiv.org/abs/2312.03543) | [![Stars](https://img.shields.io/github/stars/Petrichor625/Talk2car_CAVG?style=social)](https://github.com/Petrichor625/Talk2car_CAVG) | ‚Äî |
| **Dolphins**<br><sub>Multimodal Language Model for Driving, ] ] ]</sub><br><sub><i>Summary:</i> Dolphins is adept at processing multimodal inputs comprising video (or image) data, text instructions, and historical control signals to generate informed outputs corresponding to the provided instructions.</sub> | 2024 / ECCV | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2312.00438-b31b1b?style=flat-square)](https://arxiv.org/abs/2312.00438) | [![Stars](https://img.shields.io/github/stars/SaFoLab-WISC/Dolphins?style=social)](https://github.com/SaFoLab-WISC/Dolphins) | [Project](https://vlm-driver.github.io/) |
| **Agent-Driver**<br><sub>A Language Agent for Autonomous Driving, ] ] ]</sub><br><sub><i>Summary:</i> Agent-Driver changes the traditional autonomous driving pipeline by introducing a versatile tool library accessible through function calls, cognitive memory for common sense and experiential knowledge for decision-making, and a reasoning engine capable of thought chain reasoning, task planning, motion planning, and self-reflection.</sub> | 2024 / COLM | `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2311.10813-b31b1b?style=flat-square)](https://arxiv.org/abs/2311.10813) | [![Stars](https://img.shields.io/github/stars/USC-GVL/Agent-Driver?style=social)](https://github.com/USC-GVL/Agent-Driver) | [Project](https://usc-gvl.github.io/Agent-Driver/) |
| **Empowering Autonomous Driving with Large Language Models**<br><sub>A Safety Perspective, ] ]</sub><br><sub><i>Summary:</i> Deploys the LLM as an intelligent decision-maker in planning, incorporating safety verifiers for contextual safety learning to enhance overall AD performance and safety.</sub> | 2024 / ICLR | `VLM` ¬∑ `Planning` ¬∑ `Safety` | [![arXiv](https://img.shields.io/badge/arXiv-2312.00812-b31b1b?style=flat-square)](https://arxiv.org/abs/2312.00812) | [![Stars](https://img.shields.io/github/stars/wangyixu14/llm_conditioned_mpc_ad?style=social)](https://github.com/wangyixu14/llm_conditioned_mpc_ad) | ‚Äî |
| **ChatGPT as Your Vehicle Co-Pilot**<br><sub>An Initial Attempt ]</sub><br><sub><i>Summary:</i> Design a universal framework that embeds LLMs as a vehicle "Co-Pilot" of driving, which can accomplish specific driving tasks with human intention satisfied based on the information provided.</sub> | ‚Äî | `VLM` | [Paper](https://ieeexplore.ieee.org/document/10286969) | ‚Äî | ‚Äî |
| **Receive, Reason, and React**<br><sub>Drive as You Say with Large Language Models in Autonomous Vehicles, ]</sub><br><sub><i>Summary:</i> Utilize LLMs‚Äô linguistic and contextual understanding abilities with specialized tools to integrate the language and reasoning capabilities of LLMs into autonomous vehicles.</sub> | 2024 / ITSM | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2310.08034-b31b1b?style=flat-square)](https://arxiv.org/abs/2310.08034) | ‚Äî | ‚Äî |
| **LanguageMPC**<br><sub>Large Language Models as Decision Makers for Autonomous Driving ]</sub><br><sub><i>Summary:</i> Leverage LLMs to provide high-level decisions through chain-of-thought.Convert high-level decisions into mathematical representations to guide the bottom-level controller(MPC).</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2310.03026-b31b1b?style=flat-square)](https://arxiv.org/abs/2310.03026) | ‚Äî | ‚Äî |
| **Driving with LLMs**<br><sub>Fusing Object-Level Vector Modality for Explainable Autonomous Driving ] ]</sub><br><sub><i>Summary:</i> Propose a unique object-level multimodal LLM architecture(Llama2+Lora), using only vectorized representations as input.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2310.01957-b31b1b?style=flat-square)](https://arxiv.org/abs/2310.01957) | [![Stars](https://img.shields.io/github/stars/wayveai/Driving-with-LLMs?style=social)](https://github.com/wayveai/Driving-with-LLMs) | ‚Äî |
| **DriveGPT4**<br><sub>Interpretable End-to-end Autonomous Driving via Large Language Model, ] ]</sub><br><sub><i>Summary:</i> DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution.</sub> | RAL | `VLM` | [Paper](https://tonyxuqaq.github.io/assets/pdf/2024_RAL_DriveGPT4.pdf) | ‚Äî | [Project](https://tonyxuqaq.github.io/projects/DriveGPT4/) |
| **GPT-Driver**<br><sub>Learning to Drive with GPT, ] ] ]</sub><br><sub><i>Summary:</i> Represent the planner inputs and outputs as language tokens, and leverage the LLM to generate driving trajectories through a language description of coordinate positions.</sub> | 2023 / NeurIPS | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2310.01415-b31b1b?style=flat-square)](https://arxiv.org/abs/2310.01415) | [![Stars](https://img.shields.io/github/stars/PointsCoder/GPT-Driver?style=social)](https://github.com/PointsCoder/GPT-Driver) | [Project](https://pointscoder.github.io/projects/gpt_driver/index.html) |
| **DiLu**<br><sub>A Knowledge-Driven Approach to Autonomous Driving with Large Language Models, ] ] ]</sub><br><sub><i>Summary:</i> Propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously.</sub> | 2024 / ICLR | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2309.16292-b31b1b?style=flat-square)](https://arxiv.org/abs/2309.16292) | [![Stars](https://img.shields.io/github/stars/PJLab-ADG/DiLu?style=social)](https://github.com/PJLab-ADG/DiLu) | [Project](https://pjlab-adg.github.io/DiLu/) |
| **Drive as You Speak**<br><sub>Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles ]</sub><br><sub><i>Summary:</i> In this paper, we present a novel framework that leverages Large Language Models (LLMs) to enhance autonomous vehicles‚Äô decision-making processes. By integrating LLMs‚Äô natural language capabilities and contextual understanding, specialized tools usage, synergizing reasoning, and acting with various modules on autonomous vehicles, this framework aims to seamlessly integrate the advanced language and reasoning capabilities of LLMs into autonomous vehicles.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2309.10228-b31b1b?style=flat-square)](https://arxiv.org/abs/2309.10228) | ‚Äî | ‚Äî |
| **HiLM-D**<br><sub>Enhancing MLLMs with Multi-Scale High-Resolution Details for Autonomous Driving, ]</sub><br><sub><i>Summary:</i> ROLISP that aims to identify, explain and localize the risk object for the ego-vehicle meanwhile predicting its intention and giving suggestions. Propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task.</sub> | IJCV | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2309.05186-b31b1b?style=flat-square)](https://arxiv.org/abs/2309.05186) | ‚Äî | ‚Äî |
| **SurrealDriver**<br><sub>Designing LLM-powered Generative Driver Agent Framework based on Human Drivers' Driving-thinking Data ]</sub><br><sub><i>Summary:</i> The framework uses post-drive self-reported driving thought data from human drivers as demonstration and feedback to build a human-like generative driving agent.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2309.13193-b31b1b?style=flat-square)](https://arxiv.org/abs/2309.13193) | ‚Äî | ‚Äî |
| **Drive Like a Human**<br><sub>Rethinking Autonomous Driving with Large Language Models ] ]</sub><br><sub><i>Summary:</i> Identify three key abilities: Reasoning, Interpretation and Memorization (accumulate experience and self-reflection).</sub> | ‚Äî | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2307.07162-b31b1b?style=flat-square)](https://arxiv.org/abs/2307.07162) | [![Stars](https://img.shields.io/github/stars/PJLab-ADG/DriveLikeAHuman?tab=readme-ov-file?style=social)](https://github.com/PJLab-ADG/DriveLikeAHuman?tab=readme-ov-file) | ‚Äî |
| **ADAPT**<br><sub>Action-aware Driving Caption Transformer, ] ]</sub><br><sub><i>Summary:</i> propose a multi-task joint training framework that aligns both the driving action captioning task and the control signal prediction task.</sub> | 2023 / ICRA | `WorldModel` | [![arXiv](https://img.shields.io/badge/arXiv-2302.00673-b31b1b?style=flat-square)](https://arxiv.org/abs/2302.00673) | [![Stars](https://img.shields.io/github/stars/jxbbb/ADAPT?style=social)](https://github.com/jxbbb/ADAPT) | ‚Äî |
</details>

<p align="right">(<a href="#top">back to top</a>)</p> 
</details>

<details open>
<summary> Hybrid End-to-End Methods </summary>

### Hybrid End-to-End Methods


<details open>
<summary>2025</summary>

| üß† **Method** | üóìÔ∏è **Year / Venue** | üè∑Ô∏è **Tags** | üìÑ **Paper** | üíª **GitHub** | üåê **Project** |
|---|---|---|---|---|---|
| **AdaDrive**<br><sub>AdaDrive: Self-Adaptive Slow-Fast System for Language-Grounded Autonomous Driving, ] ]</sub><br><sub><i>Summary:</i> AdaDrive builds a self-adaptive slow‚Äìfast framework that decides when and how to invoke an LLM to assist a conventional planner in language-grounded driving. It introduces an adaptive activation loss that learns to trigger the LLM only in complex or critical scenarios, and an adaptive fusion strategy that continuously scales the LLM‚Äôs influence based on scene complexity and prediction confidence, balancing high-level reasoning with real-time efficiency.</sub> | 2025 / ICCV | `VLM` ¬∑ `WorldModel` | [![arXiv](https://img.shields.io/badge/arXiv-2511.06253-b31b1b?style=flat-square)](https://arxiv.org/abs/2511.06253) | [![Stars](https://img.shields.io/github/stars/ReaFly/AdaDrive?style=social)](https://github.com/ReaFly/AdaDrive) | ‚Äî |
| **ReAL-AD**<br><sub>Towards Human-Like Reasoning in End-to-End Autonomous Driving ]]</sub><br><sub><i>Summary:</i> ReAL-AD introduces a human-like hierarchical reasoning mechanism and innovatively integrates the vision-language model organically into the decision-making hierarchy, significantly improving the planning accuracy and safety of the end-to-end autonomous driving system.</sub> | ‚Äî | `VLM` ¬∑ `Planning` ¬∑ `Safety` | [![arXiv](https://img.shields.io/badge/arXiv-2507.12499-b31b1b?style=flat-square)](https://arxiv.org/abs/2507.12499) | ‚Äî | [Project](https://4dvlab.github.io/project_page/realad) |
| **VLAD**<br><sub>A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process, ]</sub><br><sub><i>Summary:</i> Combining a fine-tuned VLM with a state-of-the-art end-to-end system, VAD</sub> | 2025 / ITSC | `VLM` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2507.01284-b31b1b?style=flat-square)](https://arxiv.org/abs/2507.01284) | ‚Äî | ‚Äî |
| **LeAD**<br><sub>The LLM Enhanced Planning System Converged with End-to-end Autonomous Driving]</sub><br><sub><i>Summary:</i> The high-frequency E2E subsystem maintains real-time perception-planning-control cycles, while the low-frequency LLM module enhances scenario comprehension through multi-modal perception fusion with HD maps and derives optimal decisions via chain-of-thought (CoT) reasoning when baseline planners encounter capability limitations.</sub> | ‚Äî | `VLM` ¬∑ `Planning` ¬∑ `Imitation` | [![arXiv](https://img.shields.io/badge/arXiv-2507.05754-b31b1b?style=flat-square)](https://arxiv.org/abs/2507.05754) | ‚Äî | ‚Äî |
| **NetRoller**<br><sub>Interfacing General and Specialized Models for End-to-End Autonomous Driving]]</sub><br><sub><i>Summary:</i> NetRoller explores the design of adapters to facilitate seamless integration of Vision Language Model (VLM, aka GM) and Specialized Driving Model (SM)</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2506.14589-b31b1b?style=flat-square)](https://arxiv.org/abs/2506.14589) | [![Stars](https://img.shields.io/github/stars/Rex-sys-hk/NetRoller?style=social)](https://github.com/Rex-sys-hk/NetRoller) | ‚Äî |
| **SOLVE**<br><sub>Synergy of Language-Vision and End-to-End Networks for Autonomous Driving, ]</sub><br><sub><i>Summary:</i> SOLVE combines VLM and end-to-end networks through feature-level collaboration (shared visual encoder) and trajectory-level collaboration.</sub> | 2025 / CVPR | `VLM` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2505.16805-b31b1b?style=flat-square)](https://arxiv.org/abs/2505.16805) | ‚Äî | ‚Äî |
| **VERDI**<br><sub>VLM-Embedded Reasoning for Autonomous Driving]</sub><br><sub><i>Summary:</i> Align E2E perception prediction planning with VLM thought chain output, and refine VLM reasoning into E2E</sub> | ‚Äî | `VLM` ¬∑ `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2505.15925-b31b1b?style=flat-square)](https://arxiv.org/abs/2505.15925) | ‚Äî | ‚Äî |
| **ALN-P3**<br><sub>Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving]</sub><br><sub><i>Summary:</i> A unified co-refinement framework that introduces cross-modal alignment between a "fast" vision-based autonomous driving system and a "slow" language-driven reasoning module.</sub> | ‚Äî | `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2505.15158-b31b1b?style=flat-square)](https://arxiv.org/abs/2505.15158) | ‚Äî | ‚Äî |
| **VLM-E2E**<br><sub>Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion]</sub><br><sub><i>Summary:</i> The textual representation is integrated into the bird's eye view (BEV) features using VLM for semantic supervision, which enables the model to learn richer feature representations that explicitly capture the driver's attention semantics.</sub> | ‚Äî | `VLM` ¬∑ `BEV` | [![arXiv](https://img.shields.io/badge/arXiv-2502.18042-b31b1b?style=flat-square)](https://arxiv.org/abs/2502.18042) | ‚Äî | ‚Äî |
| **DIMA**<br><sub>Distilling Multi-modal Large Language Models for Autonomous Driving, ]</sub><br><sub><i>Summary:</i> Transferring knowledge from a multimodal large language model (MLLM) to a vision-based end-to-end planner via knowledge distillation</sub> | 2025 / CVPR | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2501.09757-b31b1b?style=flat-square)](https://arxiv.org/abs/2501.09757) | ‚Äî | ‚Äî |
</details>


<details open>
<summary>2024</summary>

| üß† **Method** | üóìÔ∏è **Year / Venue** | üè∑Ô∏è **Tags** | üìÑ **Paper** | üíª **GitHub** | üåê **Project** |
|---|---|---|---|---|---|
| **VLM-AD**<br><sub>End-to-End Autonomous Driving through Vision-Language Model Supervision]</sub><br><sub><i>Summary:</i> Using the reasoning annotations generated by VLM as supervisory signals, the planning accuracy of the E2E model is significantly improved and the collision rate is reduced.</sub> | ‚Äî | `VLM` ¬∑ `Planning` ¬∑ `Safety` | [![arXiv](https://img.shields.io/badge/arXiv-2412.14446-b31b1b?style=flat-square)](https://arxiv.org/abs/2412.14446) | ‚Äî | ‚Äî |
| **FASIONAD**<br><sub>FAst and Slow FusION Thinking Systems for Human-Like Autonomous Driving with Adaptive Feedback]</sub><br><sub><i>Summary:</i> The fast and slow systems of VLMAD and E2EAD, the fast system efficiently manages routine navigation tasks through fast, data-driven path planning, while the slow system handles complex reasoning and decision-making in unfamiliar or challenging scenarios.</sub> | ‚Äî | `VLM` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2411.18013-b31b1b?style=flat-square)](https://arxiv.org/abs/2411.18013) | ‚Äî | ‚Äî |
| **Senna**<br><sub>Bridging Large Vision-Language Models and End-to-End Autonomous Driving]]</sub><br><sub><i>Summary:</i> Senna is an autonomous driving system that integrates a Large Vision-Language Model with an end-to-end model to improve planning safety, robustness and generalization.</sub> | ‚Äî | `VLM` ¬∑ `Planning` ¬∑ `Safety` | [![arXiv](https://img.shields.io/badge/arXiv-2410.22313-b31b1b?style=flat-square)](https://arxiv.org/abs/2410.22313) | [![Stars](https://img.shields.io/github/stars/hustvl/Senna?style=social)](https://github.com/hustvl/Senna) | ‚Äî |
| **Hint-AD**<br><sub>Holistically Aligned Interpretability in End-to-End Autonomous Driving, ]]]</sub><br><sub><i>Summary:</i> An integrated autonomous driving-language system that generates language that is aligned with the overall perception-prediction-planning output of the autonomous driving model</sub> | 2024 / CoRL | `WorldModel` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2409.06702-b31b1b?style=flat-square)](https://arxiv.org/abs/2409.06702) | [![Stars](https://img.shields.io/github/stars/Robot-K/Hint-AD?style=social)](https://github.com/Robot-K/Hint-AD) | [Project](https://air-discover.github.io/Hint-AD/) |
| **DriveVLM**<br><sub>The Convergence of Autonomous Driving and Large Vision-Language Models, ]]</sub><br><sub><i>Summary:</i> DriveVLM-Dual, a hybrid system that incorporates DriveVLM and a traditional autonomous pipeline.</sub> | 2024 / CoRL | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2402.12289-b31b1b?style=flat-square)](https://arxiv.org/abs/2402.12289) | ‚Äî | [Project](https://tsinghua-mars-lab.github.io/DriveVLM/) |
| **DME-Driver**<br><sub>Integrating Human Decision Logic and 3D Scene Perception in Autonomous Driving, ]</sub><br><sub><i>Summary:</i> DME-Driver utilizes a powerful vision language model as the decision-maker and a planning-oriented perception model as the control signal generator.</sub> | 2025 / AAAI | `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2401.03641-b31b1b?style=flat-square)](https://arxiv.org/abs/2401.03641) | ‚Äî | ‚Äî |
| **VLP**<br><sub>Vision Language Planning for Autonomous Driving,]</sub><br><sub><i>Summary:</i> Propose VLP, a Vision Language Planning model, which is composed of novel components ALP and SLP, aiming to improve the ADS from self-driving BEV reasoning and self-driving decision-making aspects, respectively.</sub> | 2024 / CVPR | `Planning` ¬∑ `BEV` | [![arXiv](https://img.shields.io/badge/arXiv-2401.05577-b31b1b?style=flat-square)](https://arxiv.org/abs/2401.05577) | ‚Äî | ‚Äî |
</details>

</details>
<p align="right">(<a href="#top">back to top</a>)</p> 



## Dataset

<details open>
<summary>Normal Dataset</summary>

### Normal Dataset


| üß† **Method** | üóìÔ∏è **Year / Venue** | üè∑Ô∏è **Tags** | üìÑ **Paper** | üíª **GitHub** | üåê **Project** |
|---|---|---|---|---|---|
| **KITTI**<br><sub>Are We Ready for Autonomous Driving? The KITTI Vision Benchmark Suite CVPR 2012 ]]</sub><br><sub><i>Summary:</i> A total of 61 scenes were recorded using a 64-beam LiDAR, a stereo camera pair, a color camera, and GPS/IMU; the training set comprises 7,481 labeled frames and the test set comprises 7,518 frames.</sub> | ‚Äî | ‚Äî | [Paper](https://ieeexplore.ieee.org/document/6248074) | ‚Äî | [Project](https://www.cvlibs.net/datasets/kitti/) |
| **nuScenes**<br><sub>A Multimodal Dataset for Autonomous Driving CVPR 2020 ]]</sub><br><sub><i>Summary:</i> A total of 1,000 scenes (20 seconds each) were collected, including raw data from 6 cameras, 5 radars, and 1 LiDAR sensor; the training set comprises 28,130 samples, the validation set comprises 6,019 samples, and the test set comprises 6,008 samples.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-1903.11027-b31b1b?style=flat-square)](https://arxiv.org/abs/1903.11027) | ‚Äî | [Project](https://www.nuscenes.org/) |
| **Waymo Open Dataset**<br><sub>Scalability in Perception for Autonomous Driving: Waymo Open Dataset CVPR 2020 ]]</sub><br><sub><i>Summary:</i> A total of 1,150 scenes (20 seconds each) were collected, including raw data from 5 LiDAR sensors and 5 cameras; the training‚Äêvalidation set comprises 1,000 scenes, and the test set comprises 150 scenes.</sub> | ‚Äî | `Dataset` | [Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Sun_Scalability_in_Perception_for_Autonomous_Driving_Waymo_Open_Dataset_CVPR_2020_paper.pdf) | ‚Äî | [Project](https://waymo.com/intl/zh-cn/research/scalability-in-perception-for-autonomous-driving-waymo-open-dataset/) |
| **Argoverse**<br><sub>Argoverse: 3D Tracking and Forecasting with Rich Maps, CVPR 2019 ]] Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting, NeurIPS 2023 ]]</sub><br><sub><i>Summary:</i> A Sensor Dataset of 1,000 3D-annotated sequences (lidar, ring cameras, stereo cameras, HD maps), a Lidar Dataset of 20,000 unlabeled sequences, a Motion Forecasting Dataset of 250,000 scenarios, and a Map Change Dataset of 1,000 scenarios (200 depicting HD-map changes), all collected across six cities.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-1911.02620-b31b1b?style=flat-square)](https://arxiv.org/abs/1911.02620) | ‚Äî | [Project](https://www.argoverse.org/) |
| **Lyft**<br><sub>One Thousand and One Hours: Self-driving Motion Prediction Dataset ArXiv 2020 ]]</sub><br><sub><i>Summary:</i> Over 1,000 hours of driving data collected in Las Vegas and Palo Alto using a ring of 6 cameras, a 64-beam LiDAR, and GPS/IMU; annotated for agent trajectories and intent.</sub> | ‚Äî | `Dataset` ¬∑ `WorldModel` | [![arXiv](https://img.shields.io/badge/arXiv-2006.14480-b31b1b?style=flat-square)](https://arxiv.org/abs/2006.14480) | ‚Äî | [Project](https://level5.lyft.com/) |
| **ONCE**<br><sub>One Million Scenes for Autonomous Driving: ONCE Dataset NeurIPS 2021 ]]</sub><br><sub><i>Summary:</i> A million LiDAR sweeps with 28,000 frames annotated (32-beam LiDAR, 5 cameras, GNSS/IMU); training set comprises 25,000 annotated frames, validation 1,500, test 1,500.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-2106.11037-b31b1b?style=flat-square)](https://arxiv.org/abs/2106.11037) | ‚Äî | [Project](https://once-for-auto-driving.github.io/index.html) |
| **Mapillary Vistas**<br><sub>The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes ICCV 2017 ]]</sub><br><sub><i>Summary:</i> A total of 25,000 high-resolution images from diverse street scenes; training set comprises 18,000 images, validation 2,000, test 5,000; pixel-level annotations over 66 object classes.</sub> | ‚Äî | `Dataset` | [Paper](https://openaccess.thecvf.com/content_ICCV_2017/papers/Neuhold_The_Mapillary_Vistas_ICCV_2017_paper.pdf) | ‚Äî | [Project](https://www.mapillary.com/dataset/vistas) |
| **BDD100K**<br><sub>A Diverse Driving Dataset for Heterogeneous Multitask Learning CVPR 2020 ]]</sub><br><sub><i>Summary:</i> A total of 100,000 front-camera images; training set comprises 70,000 images, validation 10,000, test 20,000; annotations include bounding boxes, instance masks, drivable areas, lanes, and tracking IDs.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-1805.04687-b31b1b?style=flat-square)](https://arxiv.org/abs/1805.04687) | [![Stars](https://img.shields.io/github/stars/bdd100k/bdd100k?style=social)](https://github.com/bdd100k/bdd100k)) | [Project](https://github.com/bdd100k/bdd100k) |
| **ApolloScape**<br><sub>The ApolloScape Open Dataset for Autonomous Driving and Its Application CVPR 2018 Workshops ]]</sub><br><sub><i>Summary:</i> A total of 143,000 images with pixel-level labels and 2 million LiDAR point clouds (40-line LiDAR, 6 cameras); training set comprises 100,000 images, validation 20,000, test 23,000.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-1803.06184-b31b1b?style=flat-square)](https://arxiv.org/abs/1803.06184) | [![Stars](https://img.shields.io/github/stars/ApolloScapeAuto/dataset-api?tab=readme-ov-file?style=social)](https://github.com/ApolloScapeAuto/dataset-api?tab=readme-ov-file#data-download)) | [Project](https://github.com/ApolloScapeAuto/dataset-api?tab=readme-ov-file#data-download) |
</details>

<details open>
<summary>Vision Language Dataset</summary>

### Vision Language Dataset
<details open>
<summary>2025</summary>

| üß† **Method** | üóìÔ∏è **Year / Venue** | üè∑Ô∏è **Tags** | üìÑ **Paper** | üíª **GitHub** | üåê **Project** |
|---|---|---|---|---|---|
| **nuScenesR¬≤-6K**<br><sub>Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving]</sub><br><sub><i>Summary:</i> nuScenesR¬≤-6K is the first innovative CoT dataset for supervised fine-tuning, which effectively builds cognitive bridges between input information and output trajectories through a four-step logical chain with self-reflection for validation.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-2509.01944-b31b1b?style=flat-square)](https://arxiv.org/abs/2509.01944) | ‚Äî | ‚Äî |
| **Bench2ADVLM**<br><sub>A Closed-Loop Benchmark for Vision-language Models in Autonomous Driving]</sub><br><sub><i>Summary:</i> Bench2ADVLM, a unified hierarchical closed-loop evaluation framework for real-time, interactive assessment of ADVLMs across both simulation and physical platforms, uses routes provided in the Bench2Drive.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2508.02028-b31b1b?style=flat-square)](https://arxiv.org/abs/2508.02028) | ‚Äî | ‚Äî |
| **VLADBench**<br><sub>Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving]]]</sub><br><sub><i>Summary:</i> VLADBench is a challenging and finegrained dataset featuring close-form QAs that progress from static foundational knowledge and elements to advanced reasoning for dynamic on-road situations. The elaborate VLADBench spans 5 key domains: Traffic Knowledge Understanding, General Element Recognition, Traffic Graph Generation, Target Attribute Comprehension, and Ego Decision-Making and Planning.</sub> | ‚Äî | `Dataset` ¬∑ `VLM` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2503.21505-b31b1b?style=flat-square)](https://arxiv.org/abs/2503.21505) | [![Stars](https://img.shields.io/github/stars/Depth2World/VLADBench?tab=readme-ov-file?style=social)](https://github.com/Depth2World/VLADBench?tab=readme-ov-file)) | [Project](https://github.com/Depth2World/VLADBench?tab=readme-ov-file) |
| **NuInteract**<br><sub>Extending Large Vision-Language Model for Diverse Interactive Tasks in Autonomous Driving ]]]</sub><br><sub><i>Summary:</i> NuInteract is constructed based on nuScene. It encompasses 239K images (six single-view images and one surrounding view image for each frame) with high-quality dense captions and 1.3M data across diverse interactive language-based tasks, resulting a total of 1.5M image-text pairs.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2505.08725-b31b1b?style=flat-square)](https://arxiv.org/abs/2505.08725) | [![Stars](https://img.shields.io/github/stars/zc-zhao/DriveMonkey?style=social)](https://github.com/zc-zhao/DriveMonkey)) | [Project](https://github.com/zc-zhao/DriveMonkey) |
| **Drive-R1**<br><sub>Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning]</sub><br><sub><i>Summary:</i> Drive-R1 is designed to bridges the scenario reasoning and motion planning for AD and firstly undergoes the supervised finetuning on a elaborate dataset containing both long and short COT data. Drive-R1 is encouraged to reason step-by-step from visual input to final planning decisions.</sub> | ‚Äî | `Dataset` ¬∑ `RL` ¬∑ `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2506.18234-b31b1b?style=flat-square)](https://arxiv.org/abs/2506.18234) | ‚Äî | ‚Äî |
| **DriveAction**<br><sub>A Benchmark for Exploring Human-like Driving Decisions in VLA Models]]]</sub><br><sub><i>Summary:</i> The first action-driven benchmark specifically designed for VLA models, comprising 16,185 QA pairs generated from 2,610 driving scenarios and leveraging real-world driving data proactively collected by users of production-level autonomous vehicles.</sub> | ‚Äî | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2506.05667-b31b1b?style=flat-square)](https://arxiv.org/abs/2506.05667) | ‚Äî | [Project](https://huggingface.co/datasets/LiAuto-DriveAction/drive-action) |
| **STSBench**<br><sub>A Spatio-temporal Scenario Benchmark for Multi-modal Large Language Models in Autonomous Driving]]]</sub><br><sub><i>Summary:</i> STSnu is the first benchmark that evaluates the spatio-temporal reasoning capabilities of VLMs based on comprehensive 3D perception and evaluates driving expert VLMs for end-to-end driving, operating on videos from multi-view cameras or LiDAR.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2506.06218-b31b1b?style=flat-square)](https://arxiv.org/abs/2506.06218) | [![Stars](https://img.shields.io/github/stars/LRP-IVC/STSBench?style=social)](https://github.com/LRP-IVC/STSBench/tree/main)) | [Project](https://github.com/LRP-IVC/STSBench/tree/main) |
| **DRAMA-ROLISP**<br><sub>HiLM-D: Enhancing MLLMs with Multi-Scale High-Resolution Details for Autonomous Driving, IJCV2025. ]]]</sub><br><sub><i>Summary:</i> HiLM-D, a resource-efficient framework that enhances visual information processing in MLLMs for ROLISP (Risk Object Localization and Intention and Suggestion Prediction) .</sub> | ‚Äî | `VLM` ¬∑ `WorldModel` | [![arXiv](https://img.shields.io/badge/arXiv-2309.05186-b31b1b?style=flat-square)](https://arxiv.org/abs/2309.05186) | [![Stars](https://img.shields.io/github/stars/xmed-lab/HiLM-D?tab=readme-ov-file?style=social)](https://github.com/xmed-lab/HiLM-D?tab=readme-ov-file)) | [Project](https://github.com/xmed-lab/HiLM-D?tab=readme-ov-file) |
| **WOMD-Planning-ADE Benchmark**<br><sub>S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Model with Spatio-Temporal Visual Representation. CVPR2025. ]</sub> | ‚Äî | `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2505.24139-b31b1b?style=flat-square)](https://arxiv.org/abs/2505.24139) | ‚Äî | ‚Äî |
| **ImpromptuVLA**<br><sub>Open Weights and Open Data for Driving Vision-Language-Action Models ]]]]</sub><br><sub><i>Summary:</i> The Impromptu VLA Dataset includes over 80,000 meticulously curated video clips, distilled from over 2M source clips sourced from 8 open-source large-scale datasets.</sub> | ‚Äî | `Dataset` ¬∑ `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2505.23757-b31b1b?style=flat-square)](https://arxiv.org/abs/2505.23757) | [![Stars](https://img.shields.io/github/stars/ahydchh/Impromptu-VLA?style=social)](https://github.com/ahydchh/Impromptu-VLA)) | [Project](https://github.com/ahydchh/Impromptu-VLA) |
| **DriveBench**<br><sub>Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives. ICCV2025. ]]]]</sub><br><sub><i>Summary:</i> DriveBench is a benchmark dataset designed to evaluate VLM reliability across 17 settings, encompassing 19,200 frames, 20,498 question-answer pairs, three question types, four mainstream driving tasks, and a total of 12 popular VLMs.</sub> | ‚Äî | `Dataset` ¬∑ `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2501.04003-b31b1b?style=flat-square)](https://arxiv.org/abs/2501.04003) | [![Stars](https://img.shields.io/github/stars/drive-bench/toolkit?style=social)](https://github.com/drive-bench/toolkit)) | [Project](https://github.com/drive-bench/toolkit) |
| **SimLingo**<br><sub>Vision-Only Closed-Loop Autonomous Driving with Language-Action Alignment, CVPR2025.]]]</sub><br><sub><i>Summary:</i> Proposed model trained in this dataset that can handle three different tasks: (1) closed-loop driving, (2) vision-language understanding, and (3) language-action alignment.</sub> | ‚Äî | `Dataset` ¬∑ `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2503.09594-b31b1b?style=flat-square)](https://arxiv.org/abs/2503.09594) | [![Stars](https://img.shields.io/github/stars/RenzKa/simlingo?style=social)](https://github.com/RenzKa/simlingo)) | [Project](https://github.com/RenzKa/simlingo) |
| **WOMD-Reasoning**<br><sub>A Large-Scale Dataset for Interaction Reasoning in Driving, ICML 2025. ]]]</sub><br><sub><i>Summary:</i> WOMD-Reasoning is a large-scale Q&As dataset built on WOMD focusing on describing and reasoning traffic rule-induced interactions in driving scenariosÔºåand presents 3 million Q&As on real-world driving scenarios.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-2407.04281-b31b1b?style=flat-square)](https://arxiv.org/abs/2407.04281) | [![Stars](https://img.shields.io/github/stars/yhli123/WOMD-Reasoning?style=social)](https://github.com/yhli123/WOMD-Reasoning)) | [Project](https://github.com/yhli123/WOMD-Reasoning) |
| **OmniDrive**<br><sub>LLM-Agent for Autonomous Driving with 3D Perception, Reasoning and Planning, CVPR 2025. ]]]]</sub><br><sub><i>Summary:</i> OmniDrive is a holistic vision-language dataset that aligns agent models with 3D driving tasks through counterfactual reasoning.</sub> | ‚Äî | `Dataset` ¬∑ `VLM` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2405.01533-b31b1b?style=flat-square)](https://arxiv.org/abs/2405.01533) | [![Stars](https://img.shields.io/github/stars/NVlabs/OmniDrive?style=social)](https://github.com/NVlabs/OmniDrive)) | [Project](https://github.com/NVlabs/OmniDrive) |
| **CODA-LM**<br><sub>Automated Evaluation of Large Vision-Language Models on Self-driving Corner Cases, WACV 2025. ]]]</sub><br><sub><i>Summary:</i> CODA-LM is the very first benchmark for the automatic evaluation of LVLMs for self-driving corner cases.</sub> | ‚Äî | `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2404.10595-b31b1b?style=flat-square)](https://arxiv.org/abs/2404.10595) | [![Stars](https://img.shields.io/github/stars/DLUT-LYZ/CODA-LM?style=social)](https://github.com/DLUT-LYZ/CODA-LM)) | [Project](https://github.com/DLUT-LYZ/CODA-LM) |
| **CoVLA**<br><sub>Comprehensive Vision-Language-Action Dataset for Autonomous Driving, WACV 2025 Oral. ]]]</sub><br><sub><i>Summary:</i> CoVLA Dataset is an dataset comprising real-world driving videos spanning more than 80 hours. This dataset leverages a novel, scalable approach based on automated data processing and a caption generation pipeline to generate accurate driving trajectories paired with detailed natural language descriptions of driving environments and maneuvers.</sub> | ‚Äî | `Dataset` ¬∑ `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2408.10845-b31b1b?style=flat-square)](https://arxiv.org/abs/2408.10845) | ‚Äî | [Project](https://turingmotors.github.io/covla-ad/) |
| **nuPrompt**<br><sub>Language Prompt for Autonomous Driving. AAAI 2025. ]]]]</sub><br><sub><i>Summary:</i> NuPrompt is the first object-centric language prompt set for driving scenes within 3D, multi-view, and multi-frame space, which expands nuScenes dataset by constructing a total of 40,147 language descriptions, each referring to an average of 7.4 object tracklets.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-2309.04379-b31b1b?style=flat-square)](https://arxiv.org/abs/2309.04379) | [![Stars](https://img.shields.io/github/stars/wudongming97/Prompt4Driving?style=social)](https://github.com/wudongming97/Prompt4Driving)) | [Project](https://github.com/wudongming97/Prompt4Driving) |
| **Robusto-1 Dataset**<br><sub>Comparing Humans and VLMs on real out-of-distribution Autonomous Driving VQA from Peru]]</sub><br><sub><i>Summary:</i> The Robusto-1 dataset that uses dashcam video data from Peru, a country with one of the ‚Äúworst‚Äù (aggressive) drivers in the world, a high traffic index, and a high ratio of bizarre to non‚Äìbizarre street objects likely never seen in training. The motivation is preliminarly testing at a cognitive level how well Foundational Visual-Language Models (VLMs) compare to Humans in Driving.</sub> | ‚Äî | `Dataset` ¬∑ `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2503.07587-b31b1b?style=flat-square)](https://arxiv.org/abs/2503.07587) | ‚Äî | [Project](https://huggingface.co/datasets/Artificio/robusto-1/tree/main) |
| **DrivingVQA Dataset**<br><sub>RIV-CoT: Retrieval-Based Interleaved Visual Chain-of-Thought in Real-World Driving Scenarios. ]]]</sub><br><sub><i>Summary:</i> DrivingVQA is a visual question answering dataset derived from driving theory exams, which contains 3,931 multiple-choice problems with expert-written explanations and grounded entities relevant to the reasoning process.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-2501.04671-b31b1b?style=flat-square)](https://arxiv.org/abs/2501.04671) | [![Stars](https://img.shields.io/github/stars/vita-epfl/RIV-CoT?style=social)](https://github.com/vita-epfl/RIV-CoT)) | [Project](https://github.com/vita-epfl/RIV-CoT) |
| **DriveLMM-o1**<br><sub>A Step-by-Step Reasoning Dataset and Large Multimodal Model for Driving Scenario Understanding. ]]]]</sub><br><sub><i>Summary:</i> DriveLMM-o1 is a dataset designed to advance step-wise visual reasoning for autonomous driving, which features over 18k VQA examples in the training set and more than 4k in the test set, each enriched with step-by-step reasoning to ensure logical inference in autonomous driving scenarios.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-2503.10621-b31b1b?style=flat-square)](https://arxiv.org/abs/2503.10621) | [![Stars](https://img.shields.io/github/stars/ayesha-ishaq/DriveLMM-o1?style=social)](https://github.com/ayesha-ishaq/DriveLMM-o1)) | [Project](https://github.com/ayesha-ishaq/DriveLMM-o1) |
</details>

<details open>
<summary>2024</summary>

| üß† **Method** | üóìÔ∏è **Year / Venue** | üè∑Ô∏è **Tags** | üìÑ **Paper** | üíª **GitHub** | üåê **Project** |
|---|---|---|---|---|---|
| **DriveLM**<br><sub>Driving with Graph Visual Question Answering, ECCV 2024.]]]</sub><br><sub><i>Summary:</i> DriveLM proposed a task, Graph VQA, which is more similar to the human reasoning process. And instantiate datasets (DriveLM-Data) built upon nuScenes and CARLA, and propose a VLM-based baseline approach (DriveLM-Agent) for jointly performing Graph VQA and end-to-end driving.</sub> | ‚Äî | `Dataset` ¬∑ `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2312.14150-b31b1b?style=flat-square)](https://arxiv.org/abs/2312.14150) | [![Stars](https://img.shields.io/github/stars/OpenDriveLab/DriveLM?style=social)](https://github.com/OpenDriveLab/DriveLM)) | [Project](https://github.com/OpenDriveLab/DriveLM) |
| **LMDrive**<br><sub>Closed-Loop End-to-End Driving with Large Language Models ]]]]</sub><br><sub><i>Summary:</i> LMDrive uniquely processes and integrates multi-modal sensor data with natural language instructions, enabling interaction with humans and navigation software in realistic instructional settings. To facilitate further research in language-based closed-loop autonomous driving, the author also publicly release the corresponding dataset which includes approximately 64K instruction-following data clips, and the LangAuto benchmark that tests the system's ability to handle complex instructions and challenging driving scenarios.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-2312.07488-b31b1b?style=flat-square)](https://arxiv.org/abs/2312.07488) | [![Stars](https://img.shields.io/github/stars/opendilab/LMDrive?style=social)](https://github.com/opendilab/LMDrive)) | [Project](https://github.com/opendilab/LMDrive) |
| **DriveCoT**<br><sub>Integrating Chain-of-Thought Reasoning with End-to-End Driving ]]]]</sub><br><sub><i>Summary:</i> The author collect a comprehensive end-to-end driving dataset named DriveCoT, leveraging the CARLA simulator. It contains sensor data, control decisions, and chain-of-thought labels to indicate the reasoning process.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-2403.16996-b31b1b?style=flat-square)](https://arxiv.org/abs/2403.16996) | ‚Äî | [Project](https://drivecot.github.io/index.html) |
| **NuScenes-QA**<br><sub>A Multi-Modal Visual Question Answering Benchmark for Autonomous Driving Scenario, AAAI 2024. ]]]</sub><br><sub><i>Summary:</i> NuScenes-QA is the first benchmark for VQA in the autonomous driving scenario, encompassing 34K visual scenes and 460K question-answer pairs.</sub> | ‚Äî | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2305.14836-b31b1b?style=flat-square)](https://arxiv.org/abs/2305.14836) | [![Stars](https://img.shields.io/github/stars/qiantianwen/NuScenes-QA?style=social)](https://github.com/qiantianwen/NuScenes-QA)) | [Project](https://github.com/qiantianwen/NuScenes-QA) |
| **NuScenes-MQA**<br><sub>Integrated Evaluation of Captions and QA for Autonomous Driving Datasets using Markup Annotations, WACV2024. ]]]</sub><br><sub><i>Summary:</i> NuScenes-MQA dataset is annotated by a novel annotation technique Markup-QA,  in which QAs are enclosed within markups.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-2312.06352-b31b1b?style=flat-square)](https://arxiv.org/abs/2312.06352) | [![Stars](https://img.shields.io/github/stars/turingmotors/NuScenes-MQA?style=social)](https://github.com/turingmotors/NuScenes-MQA)) | [Project](https://github.com/turingmotors/NuScenes-MQA) |
| **Talk2BEV**<br><sub>Language-enhanced Bird‚Äôs-eye View Maps for Autonomous Driving, ICRA 2024. ]]]</sub><br><sub><i>Summary:</i> Talk2BEV-Bench, a benchmark for evaluating LVLMs for AD applications with human-annotated ground-truth for object attributes, semantics, visual reasoning, spatial understanding, and decision-making.</sub> | ‚Äî | `VLM` ¬∑ `BEV` | [![arXiv](https://img.shields.io/badge/arXiv-2310.02251-b31b1b?style=flat-square)](https://arxiv.org/abs/2310.02251) | [![Stars](https://img.shields.io/github/stars/llmbev/talk2bev?style=social)](https://github.com/llmbev/talk2bev)) | [Project](https://github.com/llmbev/talk2bev) |
| **DriveGPT4**<br><sub>Interpretable End-to-end Autonomous Driving via Large Language Model, RA-L 2024. ]]]</sub><br><sub><i>Summary:</i> DriveGPT4 develop a new visual instruction tuning dataset for interpretable autonomous driving with the assistance of ChatGPT.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-2310.01412-b31b1b?style=flat-square)](https://arxiv.org/abs/2310.01412) | ‚Äî | [Project](https://drive.google.com/drive/folders/1PsGL7ZxMMz1ZPDS5dZSjzjfPjuPHxVL5) |
| **ContextVLM**<br><sub>Zero-Shot and Few-Shot Context Understanding for Autonomous Driving using Vision Language Models, ITSC 2024. ]]]</sub><br><sub><i>Summary:</i> A large dataset named  DrivingContexts with a combination of hand-annotated  and machine annnotated labels to improve VLMs for  better context recognition.Its images are from muti-data sources.</sub> | ‚Äî | `Dataset` ¬∑ `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2409.00301-b31b1b?style=flat-square)](https://arxiv.org/abs/2409.00301) | [![Stars](https://img.shields.io/github/stars/ssuralcmu/ContextVLM?style=social)](https://github.com/ssuralcmu/ContextVLM)) | [Project](https://github.com/ssuralcmu/ContextVLM) |
| **LingoQA**<br><sub>Visual Question Answering for Autonomous Driving, ECCV 2024. ]]]</sub><br><sub><i>Summary:</i> LingoQA Datasets with 419.9k QA pair stands out with its freeform questions and answers, covering not just perception but also driving reasoning from the drivers directly, broadening the scope of autonomousdriving datasets.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-2312.14115-b31b1b?style=flat-square)](https://arxiv.org/abs/2312.14115) | [![Stars](https://img.shields.io/github/stars/wayveai/LingoQA?style=social)](https://github.com/wayveai/LingoQA)) | [Project](https://github.com/wayveai/LingoQA) |
| **Rank2Tell**<br><sub>A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning, WACV2024. ]]</sub><br><sub><i>Summary:</i> Rank2Tell, a multi-modal ego-centric dataset for Ranking the importance level and Telling the reason for the importance.Using various close and open-ended visual question answering, the dataset provides dense annotations of various semantic, spatial, temporal, and relational attributes of various important objects in complex traffic scenarios.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-2309.06597-b31b1b?style=flat-square)](https://arxiv.org/abs/2309.06597) | ‚Äî | [Project](https://usa.honda-ri.com/rank2tell) |
| **MAPLM**<br><sub>A Real-World Large-Scale Vision-Language Dataset for Map and Traffic Scene Understanding, CVPR2024. ]]]</sub><br><sub><i>Summary:</i> A new vision-language benchmark that can be used to finetune traffic and HD map domain-specific foundation models.</sub> | ‚Äî | `Dataset` ¬∑ `VLM` | [Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Cao_MAPLM_A_Real-World_Large-Scale_Vision-Language_Benchmark_for_Map_and_Traffic_CVPR_2024_paper.pdf) | [![Stars](https://img.shields.io/github/stars/LLVM-AD/MAPLM?tab=readme-ov-file?style=social)](https://github.com/LLVM-AD/MAPLM?tab=readme-ov-file#official-open-source-datasets-of-1st-workshop-on-large-language-vision-models-for-autonomous-driving-llvm-ad-in-wacv-2024)) | [Project](https://github.com/LLVM-AD/MAPLM?tab=readme-ov-file#official-open-source-datasets-of-1st-workshop-on-large-language-vision-models-for-autonomous-driving-llvm-ad-in-wacv-2024) |
| **NuInstruct**<br><sub>Holistic Autonomous Driving Understanding by Bird‚Äôs-Eye-View Injected Multi-Modal Large Models, CVPR2024. ]]]</sub><br><sub><i>Summary:</i> NuInstruct, a novel dataset with 91K multi-view video-QA pairs across 17 subtasks, where each task demands holistic information ( e.g., temporal, multi-view, and spatial), significantly elevating the challenge level.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-2401.00988-b31b1b?style=flat-square)](https://arxiv.org/abs/2401.00988) | [![Stars](https://img.shields.io/github/stars/xmed-lab/NuInstruct?style=social)](https://github.com/xmed-lab/NuInstruct)) | [Project](https://github.com/xmed-lab/NuInstruct) |
| **DriveVLMÔºàSUP-AD datasetÔºâ**<br><sub>The Convergence of Autonomous Driving and Large Vision-Language Models, CoRL 2024. ]]</sub><br><sub><i>Summary:</i> carry out a comprehensive data mining and annotation pipeline to construct an in-house SUP-AD dataset for the scene understanding and planning task.</sub> | ‚Äî | `Dataset` ¬∑ `VLM` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2402.12289-b31b1b?style=flat-square)](https://arxiv.org/abs/2402.12289) | ‚Äî | [Project](https://tsinghua-mars-lab.github.io/DriveVLM/) |
| **SURDS**<br><sub>Benchmarking Spatial Understanding and Reasoning in Driving Scenarios with Vision Language Models ]]]</sub><br><sub><i>Summary:</i> SURDS, a large-scale benchmark designed to systematically evaluate the spatial reasoning capabilities of vision language models (VLMs). Built on the nuScenes dataset, SURDS comprises 41,080 vision-question-answer training instances and 9,250 evaluation samples, spanning six spatial categories: orientation, depth estimation, pixel-level localization, pairwise distance, lateral ordering, and front-behind relations.</sub> | ‚Äî | `Dataset` ¬∑ `VLM` | [![arXiv](https://img.shields.io/badge/arXiv-2411.13112-b31b1b?style=flat-square)](https://arxiv.org/abs/2411.13112) | [![Stars](https://img.shields.io/github/stars/XiandaGuo/Drive-MLLM?style=social)](https://github.com/XiandaGuo/Drive-MLLM)) | [Project](https://github.com/XiandaGuo/Drive-MLLM) |
</details>

<details open>
<summary>2023</summary>

| üß† **Method** | üóìÔ∏è **Year / Venue** | üè∑Ô∏è **Tags** | üìÑ **Paper** | üíª **GitHub** | üåê **Project** |
|---|---|---|---|---|---|
| **DriveMLM**<br><sub>Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving ]]</sub><br><sub><i>Summary:</i> The author design an effective data engine to collect a dataset that includes decision state and corresponding explanation annotation for model training and evaluation.„ÄÇ</sub> | ‚Äî | `Dataset` ¬∑ `Planning` | [![arXiv](https://img.shields.io/badge/arXiv-2312.09245-b31b1b?style=flat-square)](https://arxiv.org/abs/2312.09245) | [![Stars](https://img.shields.io/github/stars/OpenGVLab/DriveMLM?style=social)](https://github.com/OpenGVLab/DriveMLM)) | [Project](https://github.com/OpenGVLab/DriveMLM) |
| **Reason2Drive**<br><sub>Towards Interpretable and Chain-based Reasoning for Autonomous Driving ]]]</sub><br><sub><i>Summary:</i> Reason2Drive, a benchmark dataset with over 600K video-text pairs, aimed at facilitating the study of interpretable reasoning in complex driving environments.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-2312.03661-b31b1b?style=flat-square)](https://arxiv.org/abs/2312.03661) | [![Stars](https://img.shields.io/github/stars/fudan-zvg/reason2drive?style=social)](https://github.com/fudan-zvg/reason2drive)) | [Project](https://github.com/fudan-zvg/reason2drive) |
| **Refer-KITTI dataset**<br><sub>Referring Multi-Object Tracking. CVPR2023. ]]]</sub><br><sub><i>Summary:</i> The author proposes a new and general referring understanding task, termed referring multi-object tracking (RMOT).To push forward RMOT, they construct one benchmark with scalable expressions based on KITTI, named Refer-KITTI, which provides 18 videos with 818 expressions, and each expression in a video is annotated with an average of 10.7 objects.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-2303.03366-b31b1b?style=flat-square)](https://arxiv.org/abs/2303.03366) | [![Stars](https://img.shields.io/github/stars/wudongming97/RMOT?style=social)](https://github.com/wudongming97/RMOT)) | [Project](https://github.com/wudongming97/RMOT) |
| **DRAMA**<br><sub>Joint Risk Localization and Captioning in Driving, WACV2023. ]]]</sub><br><sub><i>Summary:</i> DRAMA (Driving Risk Assessment Mechanism with A captioning module), which consists of 17,785 interactive driving scenarios collected in Tokyo, Japan. Our DRAMA dataset accommodates video- and object-level questions on driving risks with associated important objects to achieve the goal of visual captioning as a free-form language description utilizing closed and open-ended responses for multi-level questions, which can be used to evaluate a range of visual captioning capabilities in driving scenarios.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-2209.10767-b31b1b?style=flat-square)](https://arxiv.org/abs/2209.10767) | ‚Äî | [Project](https://usa.honda-ri.com/drama) |
</details>

<details open>
<summary>Before 2023</summary>

| üß† **Method** | üóìÔ∏è **Year / Venue** | üè∑Ô∏è **Tags** | üìÑ **Paper** | üíª **GitHub** | üåê **Project** |
|---|---|---|---|---|---|
| **SUTD-TrafficQA**<br><sub>A Question Answering Benchmark and an Efficient Network for Video Reasoning over Traffic Events, CVPR 2021.]]]</sub><br><sub><i>Summary:</i> SUTD-TrafficQA takes the form of video QA based on the collected 10,080 in-the-wild videos and annotated 62,535 QA pairs, for benchmarking the cognitive capability of causal inference and event understanding models in complex traffic scenarios.</sub> | ‚Äî | ‚Äî | [![arXiv](https://img.shields.io/badge/arXiv-2103.15538-b31b1b?style=flat-square)](https://arxiv.org/abs/2103.15538) | [![Stars](https://img.shields.io/github/stars/SUTDCV/SUTD-TrafficQA?style=social)](https://github.com/SUTDCV/SUTD-TrafficQA)) | [Project](https://github.com/SUTDCV/SUTD-TrafficQA) |
| **BDD-OIA dataset**<br><sub>Explainable Object-induced Action Decision for Autonomous Vehicles, CVPR 2020. ]]]</sub><br><sub><i>Summary:</i> A large dataset annotated for both driving commands and explanations.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-2003.09405-b31b1b?style=flat-square)](https://arxiv.org/abs/2003.09405) | [![Stars](https://img.shields.io/github/stars/Twizwei/bddoia_project?style=social)](https://github.com/Twizwei/bddoia_project)) | [Project](https://github.com/Twizwei/bddoia_project) |
| **HAD**<br><sub>Grounding Human-to-Vehicle Advice for Self-driving Vehicles, CVPR 2019. ]]</sub><br><sub><i>Summary:</i> The Honda Research Institute-Advice Dataset (HAD) is driving data with natural language advices. The driving videos are gathered from HDD dataset, a large-scale naturalistic driving dataset collected in San Francisco Bay Area.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-1911.06978-b31b1b?style=flat-square)](https://arxiv.org/abs/1911.06978) | ‚Äî | [Project](https://usa.honda-ri.com/HAD#Downloadthedataset) |
| **BDD-X dataset**<br><sub>Textual Explanations for Self-Driving Vehicles, ECCV 2018.]]]</sub><br><sub><i>Summary:</i> Berkeley DeepDrive eXplanation (BDD-X) dataset is a large-scale dataset with over 6,984 video clips annotated with driving descriptions.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-1807.11546-b31b1b?style=flat-square)](https://arxiv.org/abs/1807.11546) | [![Stars](https://img.shields.io/github/stars/JinkyuKimUCB/explainable-deep-driving?style=social)](https://github.com/JinkyuKimUCB/explainable-deep-driving)) | [Project](https://github.com/JinkyuKimUCB/explainable-deep-driving) |
| **Talk2Car**<br><sub>Taking Control of Your Self-Driving Car, EMNLP 2019. ]]</sub><br><sub><i>Summary:</i> The first object referral dataset that contains commands written in natural language for self-driving cars, provides a detailed comparison with related datasets such as ReferIt, RefCOCO, RefCOCO+, RefCOCOg, Cityscape-Ref and CLEVR-Ref.</sub> | ‚Äî | `Dataset` | [![arXiv](https://img.shields.io/badge/arXiv-1909.10838-b31b1b?style=flat-square)](https://arxiv.org/abs/1909.10838) | [![Stars](https://img.shields.io/github/stars/talk2car/Talk2Car?style=social)](https://github.com/talk2car/Talk2Car)) | [Project](https://github.com/talk2car/Talk2Car) |
</details>


</details>
<p align="right">(<a href="#top">back to top</a>)</p>  

## License


The GE2EAD resources is released under the Apache 2.0 license.
<p align="right">(<a href="#top">back to top</a>)</p>    


## Citation
If you find this project useful in your research, please consider citing:
```BibTeX
@article{yang2025survey,
  title={Survey of General End-to-End Autonomous Driving: A Unified Perspective},
  author={Yang, Yixiang and Han, Chuanrong and Mao, Runhao and others},
  journal={TechRxiv},
  year={2025},
  month={December},
  doi={10.36227/techrxiv.176523315.56439138/v1},
  url={https://doi.org/10.36227/techrxiv.176523315.56439138/v1}
}
```

<p align="right">(<a href="#top">back to top</a>)</p>    

<!-- links -->
[your-project-path]:AutoLab-SAI-SJTU/GE2EAD
[contributors-shield]: https://img.shields.io/github/contributors/AutoLab-SAI-SJTU/GE2EAD.svg?style=flat-square
[contributors-url]: https://github.com/AutoLab-SAI-SJTU/GE2EAD/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/AutoLab-SAI-SJTU/GE2EAD.svg?style=flat-square
[forks-url]: https://github.com/AutoLab-SAI-SJTU/GE2EAD/network/members
[stars-shield]: https://img.shields.io/github/stars/AutoLab-SAI-SJTU/GE2EAD.svg?style=flat-square
[stars-url]: https://github.com/AutoLab-SAI-SJTU/GE2EAD/stargazers
[issues-shield]: https://img.shields.io/github/issues/AutoLab-SAI-SJTU/GE2EAD.svg?style=flat-square
[issues-url]: https://img.shields.io/github/issues/AutoLab-SAI-SJTU/GE2EAD.svg
[license-shield]: https://img.shields.io/github/license/AutoLab-SAI-SJTU/GE2EAD.svg?style=flat-square
[license-url]: https://github.com/AutoLab-SAI-SJTU/GE2EAD/blob/master/LICENSE
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=flat-square&logo=linkedin&colorB=555
[linkedin-url]: https://linkedin.com/in/shaojintian